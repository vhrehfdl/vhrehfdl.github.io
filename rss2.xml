<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Hooni</title>
    <link>https://vhrehfdl.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>IT Blog</description>
    <pubDate>Wed, 25 Dec 2019 03:50:44 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Nginx + Flask + Uwsgi 초기 세팅</title>
      <link>https://vhrehfdl.github.io/2019/11/09/setting/nginx_flask_uwsgi/</link>
      <guid>https://vhrehfdl.github.io/2019/11/09/setting/nginx_flask_uwsgi/</guid>
      <pubDate>Sat, 09 Nov 2019 06:20:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Nginx + Flask + Uwsgi 초기 세팅 기록.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Nginx + Flask + Uwsgi 초기 세팅 기록.</li></ul><a id="more"></a>​<h2 id="설치-및-실행"><a href="#설치-및-실행" class="headerlink" title="설치 및 실행"></a>설치 및 실행</h2><hr><ol><li>Nginx, Flask, Uwsgi를 설치한다. <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">pip3 install nginx</span></pre></td></tr><tr><td class="code"><pre><span class="line">pip3 install uwsgi</span></pre></td></tr><tr><td class="code"><pre><span class="line">pip3 install flask</span></pre></td></tr><tr><td class="code"><pre><span class="line">pip3 install uwsgi-plugin-python</span></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>Flask 실행</p><ul><li><p>test.py 라는 파일을 만들고 실행하여 결과를 확인한다.</p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> flask <span class="hljs-keyword">import</span> Flask</span></pre></td></tr><tr><td class="code"><pre><span class="line">application = Flask(__name__)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-meta">@application.route("/")</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">return</span> <span class="hljs-string">"&lt;h1&gt;Hello!&lt;/h1&gt;"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    application.run(host=<span class="hljs-string">'0.0.0.0'</span>)</span></pre></td></tr></table></figure></li><li><p>curl 명령어를 실행해서 확인해본다.</p>  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">curl http://localhost:5000</span></pre></td></tr></table></figure></li></ul></li></ol><ol start="3"><li><p>uwsgi 파일 만들기</p> <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> test <span class="hljs-keyword">import</span> application</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    application.run()</span></pre></td></tr></table></figure><p> curl 명령어를 실행시켜 확인</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">uwsgi --socket 0.0.0.0:5000 --protocol=http -w wsgi</span></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>ini 파일을 만든다.</p><ul><li><p>매번 uwsgi 파일을 실행시킬 때 명령어를 써줄 수 없으니 파일을 만들어둔다.</p></li><li><p>socket 파일은 아래 실행 명령어를 실행시키면 자동으로 생성 되어진다.</p>  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">[uwsgi]</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-built_in">chdir</span> = /var/www/html/StoryAI/storyai/web</span></pre></td></tr><tr><td class="code"><pre><span class="line">module = wsgi</span></pre></td></tr><tr><td class="code"><pre><span class="line">socket = /tmp/myproject.sock</span></pre></td></tr><tr><td class="code"><pre><span class="line">chmod-socket = 666</span></pre></td></tr></table></figure></li><li><p>실행 명령어</p>  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">uwsgi --ini myproject.ini</span></pre></td></tr></table></figure></li></ul></li></ol><ol start="5"><li><p>nginx의 환경설정을 변경한다.</p><ul><li><p>/etc/nginx/sites-avilable/defalut</p></li><li><p>다른 예제들에서는 location @app을 설정해서 하는데 이거 인식이 잘 안되서 location / { 부분에 설정을 해주니까 되었다.</p></li><li><p>sock 파일을 통해서 서로 연결되어진다.</p></li><li><p>여기서 실수 했던게 try_files $url $url\ =404; 부분을 주석처리 안하니까 app.route를 다른걸 설정하니까 접속이 되지 않았다. 뻘짓했다.</p><center><img src="/images/setting/20191109_1520.png"></center><br></li></ul></li></ol><h2 id="참고-사이트"><a href="#참고-사이트" class="headerlink" title="참고 사이트"></a>참고 사이트</h2><hr><ul><li><p>[URL] : <a href="https://cjh5414.github.io/flask-uwsgi-nginx/" target="_blank" rel="noopener">https://cjh5414.github.io/flask-uwsgi-nginx/</a></p></li><li><p>[URL] : <a href="https://whatisthenext.tistory.com/124" target="_blank" rel="noopener">https://whatisthenext.tistory.com/124</a></p></li><li><p>[URL] : <a href="https://twpower.github.io/43-run-uwsgi-by-using-ini-file" target="_blank" rel="noopener">https://twpower.github.io/43-run-uwsgi-by-using-ini-file</a></p></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/11/09/setting/nginx_flask_uwsgi/#disqus_thread</comments>
    </item>
    
    <item>
      <title>EC2 Instance 웹서버 설치 후 구동하기</title>
      <link>https://vhrehfdl.github.io/2019/11/08/setting/aws_port_open/</link>
      <guid>https://vhrehfdl.github.io/2019/11/08/setting/aws_port_open/</guid>
      <pubDate>Fri, 08 Nov 2019 14:17:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;EC2 Webserver 설치 후 구동 기록.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>EC2 Webserver 설치 후 구동 기록.</li></ul><a id="more"></a>​<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem"></a>Problem</h2><hr><p>Nginx를 설치하고 IPv4 퍼블릭 IP 주소 값으로 접근을 했는데 되지 않았다.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><hr><p>방화벽에 80번 포트를 추가해주어야 한다.</p><center><img src="/images/setting/20191108_2317.png"></center><br><ul><li><p>Descripttion에 Security group의 launch-wizard를 선택한다.</p></li><li><p>Inbound 탭을 누른다.</p></li><li><p>Edit 버튼을 누르고 Add Rule 버튼을 클릭한다.</p><center><img src="/images/setting/20191108_2318.png"></center><br></li></ul><p>이렇게 추가하고 들어가면 끝!</p><p>접속이 될 것 이다.</p><h2 id="참고-URL"><a href="#참고-URL" class="headerlink" title="참고 URL"></a>참고 URL</h2><hr><p><a href="https://medium.com/@taeyeolkim/aws-ec2%EC%97%90-%EC%9B%B9%EC%84%9C%EB%B2%84-nginx-%EC%84%A4%EC%B9%98%ED%95%98%EA%B3%A0-%EA%B5%AC%EB%8F%99%ED%95%98%EA%B8%B0-a46a6e9484a8" target="_blank" rel="noopener">https://medium.com/@taeyeolkim/aws-ec2%EC%97%90-%EC%9B%B9%EC%84%9C%EB%B2%84-nginx-%EC%84%A4%EC%B9%98%ED%95%98%EA%B3%A0-%EA%B5%AC%EB%8F%99%ED%95%98%EA%B8%B0-a46a6e9484a8</a></p>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/11/08/setting/aws_port_open/#disqus_thread</comments>
    </item>
    
    <item>
      <title>EC2 Instance 생성 후 putty 접속</title>
      <link>https://vhrehfdl.github.io/2019/11/08/setting/aws_putty/</link>
      <guid>https://vhrehfdl.github.io/2019/11/08/setting/aws_putty/</guid>
      <pubDate>Fri, 08 Nov 2019 14:11:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;EC2에 putty로 접속하는 방법 기록.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>EC2에 putty로 접속하는 방법 기록.</li></ul><a id="more"></a>​<h2 id="AWS-EC2-Instance-생성"><a href="#AWS-EC2-Instance-생성" class="headerlink" title="AWS EC2 Instance 생성"></a>AWS EC2 Instance 생성</h2><hr><ul><li>[URL] : <a href="https://aws.amazon.com/ko/" target="_blank" rel="noopener">https://aws.amazon.com/ko/</a></li></ul><center><img src="/images/setting/20191108_2311.png"></center><br><ul><li><p>콘솔에 로그인 하기를 한 후 EC2 인스턴스를 생성한다.</p></li><li><p>사실 생성까지는 쉽지만 약간 어려울 수 있는게 보안 그룹에 Key File 부분이 헷갈릴 수 있다.</p></li><li><p>우선 .pem 확장자 파일을 다운 받는다. ( 접속 때 중요하게 사용되어진다. )</p></li></ul><h2 id="AWS-putty-접속"><a href="#AWS-putty-접속" class="headerlink" title="AWS putty 접속"></a>AWS putty 접속</h2><hr><ul><li><p>우선 puttygen을 사용해 .pem 파일을 .ppk 파일로 생성한다.</p></li><li><p>Load 버튼을 클릭한 후에 방금 만들었던 .pem 파일을 선택한다.</p><center><img src="/images/setting/20191108_2312.png" alt="load 버튼 클릭"></center><br></li><li><p>save 버튼을 누르고 이름을 지정한 후 저장한다.</p><center><img src="/images/setting/20191108_2313.png" alt="save 버튼 클릭"></center><br></li><li><p>Host Name에는 퍼블릭 DNS(IPv4)의 주소값을 입력한다.</p><center><img src="/images/setting/20191108_2314.png" alt="host name 입력"></center><br></li><li><p>접속&gt;SSH&gt;AUTH로 가서 Private Key파일을 선택한다.</p><center><img src="/images/setting/20191108_2315.png"></center><br></li></ul><ul><li><p>.ppk 파일을 세팅하고 접속한다.</p></li><li><p>EC2에 접속하면 “login as:”가 화면에 뜨는데 우분투로 설치했다면 ubuntu로 입력한다. ( 처음에 아무생각 없이 root를 했다가 안되었다. )</p></li></ul><h2 id="참고-URL"><a href="#참고-URL" class="headerlink" title="참고 URL"></a>참고 URL</h2><hr><ul><li>[URL] : <a href="https://hyeonstorage.tistory.com/271" target="_blank" rel="noopener">https://hyeonstorage.tistory.com/271</a></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/11/08/setting/aws_putty/#disqus_thread</comments>
    </item>
    
    <item>
      <title>ImportError dll load failed 지정된 모듈을 찾을 수 없습니다</title>
      <link>https://vhrehfdl.github.io/2019/09/11/error_set/pytorch_error/</link>
      <guid>https://vhrehfdl.github.io/2019/09/11/error_set/pytorch_error/</guid>
      <pubDate>Wed, 11 Sep 2019 08:33:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Error Message : ImportError: dll load failed: 지정된 모듈을 찾을 수 없습니다&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Error Message : ImportError: dll load failed: 지정된 모듈을 찾을 수 없습니다</li></ul><a id="more"></a><h2 id="문제-상황"><a href="#문제-상황" class="headerlink" title="문제 상황"></a>문제 상황</h2><hr><p>import torch는 문제가 없이 잘 되었는데…<br>import torchvision에서 에러가 터졌다.<br>지정된 모듈을 찾을 수 없다는 에러가 터졌다.</p><p>이게 어디서 자주 본 에러여서 기억을 더음어 보니…<br>tensorflow gpu 할 때 본 에러였다.</p><h2 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h2><hr><ol><li><p>파일 설치해보기</p><ul><li><p>Intel-openmp</p></li><li><p>visual studio 2017 재배포파일</p><p>Intel-openmp는 anaconda 환경에 설치하면 된다.<br>이 방법으로도 해결이 되지 않으면 2번 방법을 적용해보자.</p></li></ul></li></ol><ol start="2"><li><p>torchvision 버전 낮춰보기</p><p> 위에 방법으로 해결되지 않는다면 torchvision 버전을 0.4버전에서 0.2버전으로 낮춰보자.<br> 나는 이렇게 해서 해결했다.<br> 내 cuda 버전과 torchvision 버전이 호환되지 않아서 발생한 문제인 것 같다.</p></li></ol><h2 id="참고-URL"><a href="#참고-URL" class="headerlink" title="참고 URL"></a>참고 URL</h2><hr><ul><li>[URL] : <a href="https://mclearninglab.tistory.com/30" target="_blank" rel="noopener">https://mclearninglab.tistory.com/30</a></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/09/11/error_set/pytorch_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>tensorflow version error</title>
      <link>https://vhrehfdl.github.io/2019/04/08/error_set/tensorflow_version_error/</link>
      <guid>https://vhrehfdl.github.io/2019/04/08/error_set/tensorflow_version_error/</guid>
      <pubDate>Mon, 08 Apr 2019 05:46:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;에러 메세지 : batch() got an unexpected keyword argument ‘drop_remainder’&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;에러 원인 : Tensorflow 버전이 업그레이드 되면서 run_classifier.py의 drop_remainder 파라미터가 없어졌다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;해결 방법 : runclassfier.py 파일에서 drop_remainder 코드를 제거해줘야 한다.  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li><p>에러 메세지 : batch() got an unexpected keyword argument ‘drop_remainder’</p></li><li><p>에러 원인 : Tensorflow 버전이 업그레이드 되면서 run_classifier.py의 drop_remainder 파라미터가 없어졌다.</p></li><li><p>해결 방법 : runclassfier.py 파일에서 drop_remainder 코드를 제거해줘야 한다.  </p></li></ul><a id="more"></a><center><img src="/images/error_set/ErrorSet1_1.png" width="100%" alt="변경 전"></center><br><center><img src="/images/error_set/ErrorSet1_2.png" width="100%" alt="변경 후"></center>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/04/08/error_set/tensorflow_version_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Loaded runtime CuDNN library 7101</title>
      <link>https://vhrehfdl.github.io/2019/04/07/error_set/tensorflow_gpu_error/</link>
      <guid>https://vhrehfdl.github.io/2019/04/07/error_set/tensorflow_gpu_error/</guid>
      <pubDate>Sun, 07 Apr 2019 07:20:00 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;&lt;p&gt;에러 메세지 : Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (c&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;에
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li><p>에러 메세지 : Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (c</p></li><li><p>에러 원인 : tensorflow-gpu가 돌아가는 환경설정이  CUDA Toolkit 버전과 cuDNN 버전이 호환되지가 않아서 발생한 문제이다. </p></li><li><p>해결 방법 : Toolkit은 9.0 cuDNN은 7.0.5로 호환해주니 해결되었다.</p></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/04/07/error_set/tensorflow_gpu_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>pandas No module named &#39;pandas.core.internals.managers&#39;</title>
      <link>https://vhrehfdl.github.io/2019/04/07/error_set/pandas_error/</link>
      <guid>https://vhrehfdl.github.io/2019/04/07/error_set/pandas_error/</guid>
      <pubDate>Sun, 07 Apr 2019 03:43:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Error Message : ModuleNotFoundError: No module named ‘pandas.core.internals.managers’; ‘pandas.core.internals’ is no&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​&lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Error Message : ModuleNotFoundError: No module named ‘pandas.core.internals.managers’; ‘pandas.core.internals’ is no</li></ul><p>​<a id="more"></a></p><h2 id="발생-원인"><a href="#발생-원인" class="headerlink" title="발생 원인"></a>발생 원인</h2><hr><p>A컴퓨터에서 잘 작동했던 파이썬 코드를 B컴퓨터에서 실행시키니 저 error가 계속 발생했다.<br>처음에는 pandas 환경과 Python 버전 환경이 달라서 발생한 줄 알았다.<br>그래서 환경을 똑같이 맞추어줬는데도 계속 에러가 발생했다.<br>일단 에러 발생 부분을 보니 pickle.load() 하는 부분에서 에러가 발생했다.</p><h2 id="해결-방법"><a href="#해결-방법" class="headerlink" title="해결 방법"></a>해결 방법</h2><hr><p>결국 해결한 방법은 A 컴퓨터에서 pickle 파일 만들 때 pandas.DataFrame으로 넣지 않고  pandas.to_dict()로 바꿔서 파일을 만들었다.<br>B 컴퓨터에서 pickle 파일 불러오고 dict 파일을 pandas.DataFrame으로 넣어서 해결했다.<br>pickle 파일에 데이터 넣을 때 함부로 넣으면 안 되나 보다.</p>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/04/07/error_set/pandas_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Tensorflow RuntimeError Missing implementation that supports loader</title>
      <link>https://vhrehfdl.github.io/2019/04/04/error_set/tensorflow_hub_error-1/</link>
      <guid>https://vhrehfdl.github.io/2019/04/04/error_set/tensorflow_hub_error-1/</guid>
      <pubDate>Thu, 04 Apr 2019 02:44:00 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;&lt;p&gt;에러 메세지 : RuntimeError: Missing implementation that supports: loader(*(‘/tmp/tfhub_modules/mobilenet_module’,)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;해
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li><p>에러 메세지 : RuntimeError: Missing implementation that supports: loader(*(‘/tmp/tfhub_modules/mobilenet_module’,)</p></li><li><p>해결 방법 : loader 뒤에 뜨는 에러메세지 경로에 들어가서 tf_hub 파일을 지워주고 다시 실행시키니 작동하였다.<br>이게 갑자기 어느 순간에 에러가 발생하는데… 왜 발생하는지는 모르겠다.<br>그래도 해결방법이 단순하다.</p></li><li><p>참고 URL<br><a href="https://stackoverflow.com/questions/54029556/how-to-fix-runtimeerror-missing-implementation-that-supports-loader-when-cal" target="_blank" rel="noopener">https://stackoverflow.com/questions/54029556/how-to-fix-runtimeerror-missing-implementation-that-supports-loader-when-cal</a></p></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/04/04/error_set/tensorflow_hub_error-1/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Tensorflow ImportError cannot import name &#39;regex_replace&#39;</title>
      <link>https://vhrehfdl.github.io/2019/03/27/error_set/tensorflow_hub_error/</link>
      <guid>https://vhrehfdl.github.io/2019/03/27/error_set/tensorflow_hub_error/</guid>
      <pubDate>Wed, 27 Mar 2019 04:41:00 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;해결 방법 : ELMo 사용하려고 Tensorflow Hub를 설치하고 실행시켜보니 위에 error 발생!&lt;br&gt;처음에 0.3 버전으로 설치했는데…&lt;br&gt;혹시나 해서 버전 0.2로 낮춰서 해보니 실행이 되었다… &lt;/li&gt;
&lt;/ul&gt;

        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>해결 방법 : ELMo 사용하려고 Tensorflow Hub를 설치하고 실행시켜보니 위에 error 발생!<br>처음에 0.3 버전으로 설치했는데…<br>혹시나 해서 버전 0.2로 낮춰서 해보니 실행이 되었다… </li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/03/27/error_set/tensorflow_hub_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Google Colab 사용법 정리</title>
      <link>https://vhrehfdl.github.io/2019/03/22/setting/colab_tutorial/</link>
      <guid>https://vhrehfdl.github.io/2019/03/22/setting/colab_tutorial/</guid>
      <pubDate>Fri, 22 Mar 2019 02:19:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;colab 기초 사용법&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>colab 기초 사용법</li></ul><a id="more"></a>​<h2 id="Tutorial"><a href="#Tutorial" class="headerlink" title="Tutorial"></a>Tutorial</h2><hr><ol><li><p>Colab에 접속한다.</p><ul><li>[URL] : <a href="https://colab.research.google.com/notebooks/welcome.ipynb" target="_blank" rel="noopener">https://colab.research.google.com/notebooks/welcome.ipynb</a><br>​</li></ul></li><li><p>새로운 python3.ipynb 파일을 만든 후 google drive mount를 한다.</p><ul><li><p>해당 코드를 실행하면 enter authorization code를 입력하라고 창이 뜬다.</p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> google.colab <span class="hljs-keyword">import</span> drive</span></pre></td></tr><tr><td class="code"><pre><span class="line">drive.mount(<span class="hljs-string">'/content/gdrive'</span>)</span></pre></td></tr></table></figure></li></ul></li></ol><ol start="3"><li><p>authorization code 받는 주소</p><ul><li><p>[URL] : <a href="https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH" target="_blank" rel="noopener">https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH</a><br>​</p></li><li><p>해당 URL에 들어가면 최종적으로 authorization code를 받을 수 있다.</p><center><img src="/images/setting/20190322_1119.png" alt="tensorboard 실행 이미지"></center><br></li></ul></li></ol><ol start="4"><li><p>foo.txt 파일을 드라이브에 업로드</p> <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">with</span> open(<span class="hljs-string">'/content/gdrive/My Drive/foo.txt'</span>, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> f:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    f.write(<span class="hljs-string">'Hello Google Drive!'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">!cat /content/gdrive/My\ Drive/foo.txt</span></pre></td></tr></table></figure><p>​</p></li></ol>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/03/22/setting/colab_tutorial/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Tensorboard 사용법</title>
      <link>https://vhrehfdl.github.io/2019/03/08/setting/tensorboard_setting/</link>
      <guid>https://vhrehfdl.github.io/2019/03/08/setting/tensorboard_setting/</guid>
      <pubDate>Fri, 08 Mar 2019 08:24:00 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;생각&quot;&gt;&lt;a href=&quot;#생각&quot; class=&quot;headerlink&quot; title=&quot;생각&quot;&gt;&lt;/a&gt;생각&lt;/h2&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;처음에 아무 생각 없이 하다가 내 tensorflow를 아나콘다 가상환경에 설치한 사실을 깜빡하고 tensorboard를 실행 못시켰었다. &lt;/li&gt;
&lt;li&gt;tensorflow가 설치되어 있는 가상환경을 activate하고 tensorboard를 실행시키자. ㅋㅋ&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="생각"><a href="#생각" class="headerlink" title="생각"></a>생각</h2><hr><ul><li>처음에 아무 생각 없이 하다가 내 tensorflow를 아나콘다 가상환경에 설치한 사실을 깜빡하고 tensorboard를 실행 못시켰었다. </li><li>tensorflow가 설치되어 있는 가상환경을 activate하고 tensorboard를 실행시키자. ㅋㅋ</li></ul><a id="more"></a>​<h2 id="설치-및-실행"><a href="#설치-및-실행" class="headerlink" title="설치 및 실행"></a>설치 및 실행</h2><hr><ol><li><p>tensorboard 설치 </p><ul><li>기본적으로 tensorflow 설치하면 자동으로 tensorboard가 설치되어진다.</li></ul></li><li><p>mygraph라는 폴더를 만들어서 log를 저장한다.</p> <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">a = tf.constant(<span class="hljs-number">5</span>, name=<span class="hljs-string">'input_a'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">b = tf.constant(<span class="hljs-number">7</span>, name=<span class="hljs-string">'input_b'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">c = tf.multiply(a, b, name=<span class="hljs-string">'mul_c'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">d = tf.add(a, b, name=<span class="hljs-string">'add_d'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">e = tf.add(c, d, name=<span class="hljs-string">'add_e'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">sess = tf.Session()</span></pre></td></tr><tr><td class="code"><pre><span class="line">print(sess.run(e))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">wirter = tf.summary.FileWriter(<span class="hljs-string">'./mygraph'</span>,sess.graph)</span></pre></td></tr></table></figure></li><li><p>mygraph가 위치하는 폴더에 가서 아래의 명령어를 실행시켜준다.</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=<span class="hljs-string">"mygraph"</span></span></pre></td></tr></table></figure></li><li><p>커맨드창 가장 하단에 있는 <a href="http://localhost:6006으로" target="_blank" rel="noopener">http://localhost:6006으로</a> 들어가면 tensorboard가 실행되어진다.</p> <center><img src="/images/setting/20190308_1724.png" alt="tensorboard 실행 이미지"></center><br></li><li><p>최종 결과</p> <center><img src="/images/setting/20190308_1725.png" alt="tensorboard 실행 이미지"></center><br>​</li><li><p>응용</p></li></ol><ul><li><p>아래의 코드는 callback 함수로 만들어서 log를 쌓는 방법이다.</p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">from</span> keras.callbacks <span class="hljs-keyword">import</span> TensorBoard</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">create_callbacks</span><span class="hljs-params">()</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    tensorboard_callback = TensorBoard(log_dir=<span class="hljs-string">'./mygraph/mlp'</span>, histogram_freq=<span class="hljs-number">1</span>, batch_size=<span class="hljs-number">32</span>, write_graph=<span class="hljs-literal">True</span>, write_grads=<span class="hljs-literal">False</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">return</span> [tensorboard_callback]</span></pre></td></tr></table></figure></li><li><p>위의 코드를 만든후에 model을 실행시킬 때 콜백으로 넣어주면 log가 실시간으로 쌓인다.</p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line">callbacks = create_callbacks()</span></pre></td></tr><tr><td class="code"><pre><span class="line">model.fit(x=data[<span class="hljs-string">"train_X"</span>], y=data[<span class="hljs-string">"train_y"</span>], batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">200</span>, verbose=<span class="hljs-number">1</span>, validation_data=(data[<span class="hljs-string">"val_X"</span>], data[<span class="hljs-string">"val_y"</span>]), callbacks=callbacks)</span></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/03/08/setting/tensorboard_setting/#disqus_thread</comments>
    </item>
    
    <item>
      <title>python encoding error</title>
      <link>https://vhrehfdl.github.io/2019/03/06/error_set/python_encoding_error/</link>
      <guid>https://vhrehfdl.github.io/2019/03/06/error_set/python_encoding_error/</guid>
      <pubDate>Wed, 06 Mar 2019 01:51:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;에러 메세지 : UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 0: invalid&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;해결 방법 : encoding 부분을 utf-8에서 utf-16으로 바꿔주니까 된다. ( python3 기준 )&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li><p>에러 메세지 : UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 0: invalid</p></li><li><p>해결 방법 : encoding 부분을 utf-8에서 utf-16으로 바꿔주니까 된다. ( python3 기준 )</p></li></ul><a id="more"></a><ul><li><p>예시 코드</p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-comment"># -*- coding: utf-8 -*-:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> csv</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">f = open(<span class="hljs-string">'1_50000.csv'</span>, <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-16'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">rdr = csv.reader(f)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> rdr:</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(line)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">f.close()</span></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/03/06/error_set/python_encoding_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy AttributeError module &#39;lib&#39; has no attribute &#39;Cryptography_HAS_SSL_ST</title>
      <link>https://vhrehfdl.github.io/2019/02/26/error_set/scrapy_error/</link>
      <guid>https://vhrehfdl.github.io/2019/02/26/error_set/scrapy_error/</guid>
      <pubDate>Tue, 26 Feb 2019 10:30:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Error Message : AttributeError module ‘lib’ has no attribute ‘Cryptography_HAS_SSL_ST&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;원인&quot;&gt;&lt;a href=&quot;#원인&quot; class=&quot;headerlink&quot; title=&quot;원인&quot;&gt;&lt;/a&gt;원인&lt;/h2&gt;&lt;hr&gt;
&lt;p&gt;Scrapy 설치하고 실행하려는데 에러가 발생했었다.&lt;br&gt;찾아보니까 pyOpenSSL 문제인 것 같다.&lt;br&gt;그냥 install 하나 해주니까 해결되었다. &lt;/p&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Error Message : AttributeError module ‘lib’ has no attribute ‘Cryptography_HAS_SSL_ST</li></ul><h2 id="원인"><a href="#원인" class="headerlink" title="원인"></a>원인</h2><hr><p>Scrapy 설치하고 실행하려는데 에러가 발생했었다.<br>찾아보니까 pyOpenSSL 문제인 것 같다.<br>그냥 install 하나 해주니까 해결되었다. </p><a id="more"></a><h2 id="해결방법"><a href="#해결방법" class="headerlink" title="해결방법"></a>해결방법</h2><hr><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">pip3 install pyOpenSSL==0.15.1</span></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/26/error_set/scrapy_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Pytube Sample Code</title>
      <link>https://vhrehfdl.github.io/2019/02/25/setting/pytube_sample/</link>
      <guid>https://vhrehfdl.github.io/2019/02/25/setting/pytube_sample/</guid>
      <pubDate>Mon, 25 Feb 2019 02:36:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Python을 이용한 Youtube 다운로드 라이브러리.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Python을 이용한 Youtube 다운로드 라이브러리.</li></ul><a id="more"></a>​<ol><li><p>pip 툴을 사용해 pytube를 install 한다.</p></li><li><p>code를 돌려 youtube 영상을 다운 받는다.</p></li></ol><figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-comment"># -*- coding: utf-8 -*-</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> os</span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> subprocess</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">import</span> pytube</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">yt = pytube.YouTube(<span class="hljs-string">"https://www.youtube.com/watch?v=ZMpQgvmw3kk"</span>) <span class="hljs-comment">#다운받을 동영상 URL 지정</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">vids= yt.streams.all()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-comment">#영상 형식 리스트 확인</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(vids)):</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(i,<span class="hljs-string">'. '</span>,vids[i])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">vnum = int(input(<span class="hljs-string">"다운 받을 화질은? "</span>))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">parent_dir = <span class="hljs-string">"/home/lee"</span> <span class="hljs-comment">#저장 경로 지정(Windows or mac)</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">vids[vnum].download(parent_dir) <span class="hljs-comment">#다운로드 수행</span></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">default_filename = vids[vnum].default_filename</span></pre></td></tr><tr><td class="code"><pre><span class="line">subprocess.call([<span class="hljs-string">'ffmpeg'</span>, <span class="hljs-string">'-i'</span>,                 <span class="hljs-comment">#cmd 명령어 수행</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    os.path.join(parent_dir, default_filename)</span></pre></td></tr><tr><td class="code"><pre><span class="line">])</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">print(<span class="hljs-string">'동영상 다운로드 및 mp3 변환 완료!'</span>)</span></pre></td></tr></table></figure><p>​</p>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/25/setting/pytube_sample/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Google Speech API 아무것도 출력이 되지 않는 에러</title>
      <link>https://vhrehfdl.github.io/2019/02/25/error_set/google_speech_api_error/</link>
      <guid>https://vhrehfdl.github.io/2019/02/25/error_set/google_speech_api_error/</guid>
      <pubDate>Mon, 25 Feb 2019 02:32:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;원인&lt;br&gt;아래 코드의 sample_rate_hertz가 일치하지 않아서 그렇다.&lt;br&gt;ffmpeg를 사용해 동영상 확장자를 변환할 때 맞춰주어야 한다.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>원인<br>아래 코드의 sample_rate_hertz가 일치하지 않아서 그렇다.<br>ffmpeg를 사용해 동영상 확장자를 변환할 때 맞춰주어야 한다.</li></ul><a id="more"></a><hr><ul><li><p>해결방법<br>  ffmpeg로 변환을 시킨다.<br>  sample_rate_hertz가 16000으로 설정했기 때문에 변활시킬 때 16000으로 바꿔주어야 한다.</p>  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">ffmpeg -i audio_file.mp4 -acodec pcm_s16le -ac 1 -ar 16000 audio_file.wav</span></pre></td></tr></table></figure></li></ul><ul><li><p>예제 코드  </p>  <figure class="highlight python hljs"><table><tr><td class="code"><pre><span class="line"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">transcribe_file</span><span class="hljs-params">(speech_file)</span>:</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-string">"""Transcribe the given audio file."""</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">from</span> google.cloud <span class="hljs-keyword">import</span> speech</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">from</span> google.cloud.speech <span class="hljs-keyword">import</span> enums</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">from</span> google.cloud.speech <span class="hljs-keyword">import</span> types</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    client = speech.SpeechClient()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">with</span> io.open(speech_file, <span class="hljs-string">'rb'</span>) <span class="hljs-keyword">as</span> audio_file:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        content = audio_file.read()</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    audio = types.RecognitionAudio(content=content)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    config = types.RecognitionConfig(</span></pre></td></tr><tr><td class="code"><pre><span class="line">        encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,</span></pre></td></tr><tr><td class="code"><pre><span class="line">        sample_rate_hertz=<span class="hljs-number">16000</span>,</span></pre></td></tr><tr><td class="code"><pre><span class="line">        language_code=<span class="hljs-string">'ko-KR'</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">    response = client.recognize(config, audio)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    print(<span class="hljs-string">"test2"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-comment"># Each result is for a consecutive portion of the audio. Iterate through</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-comment"># them to get the transcripts for the entire audio file.</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">    <span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> response.results:</span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(<span class="hljs-string">"test3"</span>)</span></pre></td></tr><tr><td class="code"><pre><span class="line">        <span class="hljs-comment"># The first alternative is the most likely one for this portion.</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">        print(<span class="hljs-string">u'Transcript: &#123;&#125;'</span>.format(result.alternatives[<span class="hljs-number">0</span>].transcript))</span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line"></span></pre></td></tr><tr><td class="code"><pre><span class="line">speech_file = <span class="hljs-string">"audio_file.wav"</span></span></pre></td></tr><tr><td class="code"><pre><span class="line">transcribe_file(speech_file)</span></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/25/error_set/google_speech_api_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Docker 설치하기</title>
      <link>https://vhrehfdl.github.io/2019/02/24/setting/docker_install/</link>
      <guid>https://vhrehfdl.github.io/2019/02/24/setting/docker_install/</guid>
      <pubDate>Sun, 24 Feb 2019 06:35:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Docker 설치와 기본 명령어 기록.&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Docker 설치와 기본 명령어 기록.</li></ul><a id="more"></a>​<h2 id="Docker-설치"><a href="#Docker-설치" class="headerlink" title="Docker 설치"></a>Docker 설치</h2><hr><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://get.docker.com/ | sudo sh</span></pre></td></tr></table></figure><p>​</p><h2 id="설치-확인"><a href="#설치-확인" class="headerlink" title="설치 확인"></a>설치 확인</h2><hr><figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker version</span></pre></td></tr></table></figure><ul><li>설치가 완료되면 아래과 같이 결과가 나온다.</li></ul><center><img src="/images/setting/20190224_1535.png" alt="설치 완료"></center><br>​<h2 id="docker-명령어-모음"><a href="#docker-명령어-모음" class="headerlink" title="docker 명령어 모음"></a>docker 명령어 모음</h2><hr><ol><li>Image 파일 설치 :  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker run ubuntu:16.04</span></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>docker run </p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker run ubuntu:16.04</span></pre></td></tr></table></figure></li><li><p>docker exec : 실행중인 컨테이너에 접속할 때 사용</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker <span class="hljs-built_in">exec</span> -it mysql bash</span></pre></td></tr></table></figure></li><li><p>docker image 확인</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker images</span></pre></td></tr></table></figure></li><li><p>docker container 확인</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker ps -a</span></pre></td></tr></table></figure></li><li><p>docker image 제거</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker rmi <span class="hljs-string">'image id'</span></span></pre></td></tr></table></figure></li><li><p>docker container 제거</p> <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">docker rm <span class="hljs-string">'conatiner id'</span></span></pre></td></tr></table></figure><p>​</p></li></ol><h2 id="참고한-URL"><a href="#참고한-URL" class="headerlink" title="참고한 URL"></a>참고한 URL</h2><ul><li>[URL] : <a href="https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html" target="_blank" rel="noopener">https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html</a></li><li>매우 잘 설명 되어 있고 예제 코드도 많다.</li></ul><br><ul><li>[URL] : <a href="http://pyrasis.com/book/DockerForTheReallyImpatient/Chapter20/08" target="_blank" rel="noopener">http://pyrasis.com/book/DockerForTheReallyImpatient/Chapter20/08</a></li><li>Exec 명령어 설명되어 있다.</li></ul><br><ul><li>[URL] : <a href="https://blusky10.tistory.com/362" target="_blank" rel="noopener">https://blusky10.tistory.com/362</a></li><li>docker mysql 설치할 때 참조했다.</li></ul><br><ul><li>[URL] : <a href="https://behonestar.tistory.com/85" target="_blank" rel="noopener">https://behonestar.tistory.com/85</a></li><li>docker 전체 삭제 명령어</li></ul><br><ul><li>[URL] : <a href="https://blog.hanumoka.net/2018/04/29/docker-20180429-docker-install-mysql/" target="_blank" rel="noopener">https://blog.hanumoka.net/2018/04/29/docker-20180429-docker-install-mysql/</a></li><li>마찬가지로 mysql 설치할 때 참조했다.<br>​</li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/24/setting/docker_install/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Ubuntu E Problem executing scripts</title>
      <link>https://vhrehfdl.github.io/2019/02/24/error_set/ubuntu_error/</link>
      <guid>https://vhrehfdl.github.io/2019/02/24/error_set/ubuntu_error/</guid>
      <pubDate>Sun, 24 Feb 2019 03:22:00 GMT</pubDate>
      <description>
      
        &lt;ul&gt;
&lt;li&gt;Error : Ubuntu E: Problem executing scripts APT&lt;br&gt;우분투에서 apt-get 사용하다 발생한 error&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>Error : Ubuntu E: Problem executing scripts APT<br>우분투에서 apt-get 사용하다 발생한 error</li></ul><a id="more"></a><ul><li><p>Solution </p>  <figure class="highlight bash hljs"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove libappstream3</span></pre></td></tr></table></figure><p>  쉽게 해결된다.</p></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/24/error_set/ubuntu_error/#disqus_thread</comments>
    </item>
    
    <item>
      <title>BERT</title>
      <link>https://vhrehfdl.github.io/2019/02/23/word_embedding/bert/</link>
      <guid>https://vhrehfdl.github.io/2019/02/23/word_embedding/bert/</guid>
      <pubDate>Sat, 23 Feb 2019 07:58:00 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;BERT-소개&quot;&gt;&lt;a href=&quot;#BERT-소개&quot; class=&quot;headerlink&quot; title=&quot;BERT 소개&quot;&gt;&lt;/a&gt;BERT 소개&lt;/h2&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Google 에서 만든 Word Embedding 기법 ( 2018. 10. 11 논문 공개 )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NLP 11개 Task에 SOTA(State of the Arts)를 기록했으며, SQuAD v1.1에서는 인간보다 더 높은 정확도를 보여 주목을 받고 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;최근까지 GLUE NLP Task 에서 1등을 차지했었다. ( 그러나 MT-DNN에 1등을 뺏겼다. )&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pre-trained 기반 딥러닝 언어 모델&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BERT 개발자들의 접근방식 : (1) 범용 솔루션을 (2) 스케일러블 한 형태로 구현해서 (3) 많은 머신리소스로 훈련해서 성능을 높인다&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;BERT는 Contextual Embedding 방법에 속한다. (Contextualised Word Embedding은 단어마다 벡터가 고정되어 있지 않고 문장마다 단어의 Vector가 달라지는 Embedding 방법을 뜻한다 대표적으로 ELMo, GPT, BERT가 있다.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="BERT-소개"><a href="#BERT-소개" class="headerlink" title="BERT 소개"></a>BERT 소개</h2><hr><ul><li><p>Google 에서 만든 Word Embedding 기법 ( 2018. 10. 11 논문 공개 )</p></li><li><p>NLP 11개 Task에 SOTA(State of the Arts)를 기록했으며, SQuAD v1.1에서는 인간보다 더 높은 정확도를 보여 주목을 받고 있다.</p></li><li><p>최근까지 GLUE NLP Task 에서 1등을 차지했었다. ( 그러나 MT-DNN에 1등을 뺏겼다. )</p></li><li><p>Pre-trained 기반 딥러닝 언어 모델</p></li><li><p>BERT 개발자들의 접근방식 : (1) 범용 솔루션을 (2) 스케일러블 한 형태로 구현해서 (3) 많은 머신리소스로 훈련해서 성능을 높인다</p></li><li><p>BERT는 Contextual Embedding 방법에 속한다. (Contextualised Word Embedding은 단어마다 벡터가 고정되어 있지 않고 문장마다 단어의 Vector가 달라지는 Embedding 방법을 뜻한다 대표적으로 ELMo, GPT, BERT가 있다.)</p></li></ul><a id="more"></a>​​<h2 id="Static-Word-Embedding-문제점"><a href="#Static-Word-Embedding-문제점" class="headerlink" title="Static Word Embedding 문제점"></a>Static Word Embedding 문제점</h2><hr><ul><li><p>Static Word Embedding은 단어마다 벡터가 고정되어 있는 방법을 뜻하면 대표적으로 Word2vec, Fasttext, Glove이 존재한다.</p></li><li><p>Problem : 단어의 Vector가 모든 문맥에서 동일하다.</p></li><li><p>에를 들어 “배를 타고 떠났다”와 “맛있는 배를 먹었다”라는 문장에서 “배를”은 같은 벡터 값을 가진다.</p></li></ul><center><img src="/images/word_embedding/20190219_1658.png" alt="출처 : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/"></center><br><ul><li><p>“배를” 이란 단어는 One-hot Encoding 방식으로 표현하면 [ 0 0 0 1 0 ]의 값을 가진다. </p></li><li><p>첫번째 문장에서도 [ 0 0 0 1 0 ]을 가지고 두번째 문장에서도 [ 0 0 0 1 0 ]을 가진다.<br>모든 문장에서 고정된 One-hot Encoding 값을 가진다!</p></li><li><p>모든 단어들이 고정된 One-hot Encoding을 가지는 상태에서 Weight Vector를 곱하면 모든 문장에서 똑같은 Vector 값을 가지게 된다. ( 그림1을 보고 충분히 생각해보면 왜 고정된 Embedding Vector 값이 나오는지 알 수 있을 것이다. )</p></li><li><p>그리고 Static Word Embedding 에서는 Shallow Neural Net을 사용해서 학습을 진행했었다.<br>Shallow Neural Net은 LSTM이나 RNN과 같이 순환 신경망 계열이 아니기 때문에 문맥 정보가 반영되지 않고 학습이 진행된다.</p></li></ul><h2 id="Contextual-Language-Model"><a href="#Contextual-Language-Model" class="headerlink" title="Contextual Language Model"></a>Contextual Language Model</h2><hr><ol><li><p>Semi-Supervised Sequence Learning, Google, 2015</p> <center><img src="/images/word_embedding/20190219_1659.png" alt="(그림1) Sequence Learning 구조"></center><br><ul><li>Shallow Neural Net이 아닌 RNN, LSTM, GRU와 같은 순환신경망은 이전 정보를 반영해서 학습을 진행한다.</li></ul></li><li><p>ELMo : Deep Contextual Word Embeddings, AI2&amp;University of Washington, 2017</p> <center><img src="/images/word_embedding/20190219_1700.png" alt="(그림2) ELMo 구조"></center><br><ul><li><p>문장의 좌측에서 우측으로 우측에서 좌측으로 따로 따로 학습한다.</p></li><li><p>왜냐하면 문맥정보를 조금더 많이 반영하기 위해서 전-&gt;후 뿐만 아니라 후-&gt;전으로도 학습을 진행한다.</p></li></ul></li></ol><ol start="3"><li><p>GPT : Improving Language Understanding by Generative Pre-Training, OpenAI, 2018</p> <center><img src="/images/word_embedding/20190219_1701.png" alt="(그림3) GPT 구조"></center><br><ul><li>LSTM이 아닌 Transformer를 사용해 학습을 진행한다.<br>​</li></ul></li><li><p>BERT</p> <center><img src="/images/word_embedding/20190219_1702.png" alt="(그림4) BERT 구조"></center><br><ul><li><p>BERT는 GPT와 같이 Transformer를 이용하여 ELMo와 같이 양방향으로 학습을 진행한다.</p></li><li><p>사람도 언어를 이해할 때 양방향으로 이해하기 때문에 양방향으로 학습을 진행하면 성능이 좋아질 것이라 가정. ( BERT의 가장 큰 특징이라고 할 수 있다. )</p></li><li><p>ELMo와는 다르다. ELMo는 단방향으로 좌측 우측 각 각 학습하는 것이고 BERT는 양방향으로 동시에 학습을 진행하는 것이다.</p></li><li><p>하지만 양방향 학습을 하게 되면 단어가 자기 자신을 참조하는 문제가 발생한다.</p></li><li><p>“However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model.” </p></li><li><p>예를 들어 “배를 타고 떠났다.”라는 문장이 있고 단방향 학습을 진행한다고 가정해보자.<br>“타고” 라는 단어를 학습할 때 “배를”이라는 단어만을 사용해서 학습을 진행한다.<br>자기 자신을 참조할 일이 전혀 없다. </p></li><li><p>(그림5)를 보게 되면 빨간 박스 부분 학습에 영향을 미치는 Layer는 파란색 박스 밖에 없다.<br>​</p><center><img src="/images/word_embedding/20190219_1703.png" alt="(그림5) 단방향 모델"></center><br></li><li><p>하지만 양방향 학습 같은 경우에는 영향을 미친다.</p></li><li><p>(그림6) 을 보면 빨간 박스에 직접적으로 영향을 미치는 것들은 파란색 박스이다.</p></li><li><p>하지만 파란색 박스에 초록색 박스 또한 영향을 미치고 있다.</p></li><li><p>“타고”라는 단어 또한 직접적이지는 않지만 간적적으로 학습에 영향을 미친다는 것이다.</p><center><img src="/images/word_embedding/20190219_1704.png" alt="(그림6) 양방향 모델"></center><br></li></ul></li></ol><h2 id="BERT-특징"><a href="#BERT-특징" class="headerlink" title="BERT 특징"></a>BERT 특징</h2><hr><ol><li><p>Masked LM</p><ul><li><p>(그림7)과 같이 자기 자신을 참조하는 문제를 해결하기 위해 MASK를 씌워 학습을 진행한다.<br>이러한 방법이 오래전부터 문맥을 파악하는데 제시되었던 방법이라고 한다.</p><center><img src="/images/word_embedding/20190219_1705.png" alt="(그림7)"></center><br></li><li><p>Masked LM을 사용하게 되면 또 다른 문제가 발생하게 된다.</p></li><li><p>실제 Fine-tuning과정에 학습 데이터에는 Mask toeknd이 존재하지 않는다는 것이다.</p></li><li><p>문장에 MASK를 씌우는 거는 Pre-trained 학습에서만 하지 실제 fine-tuning 할 때는 씌우지 않기 때문에 간극이 발생할 것이라 생각했다고 한다.</p></li><li><p>그에 대한 해결책으로 Mask token의 80%는 mask로 10%는 random word로 10%는 unchanged word로 넣어준다. ( 즉, 일부러 Noise를 넣음으로서 너무 Deep 한 모델에서 발생할 수 있는 Over-fitting문제를 회피하도록 의도하였다 )</p></li><li><p>예를 들면 아래와 같이 진행했다.</p><p>  Mask token의 80% : 내 개는 크다 -&gt; 내 개는 [MASK]</p><p>  Mask token의 10% : 내 개는 크다 -&gt; 내 개는 사과</p><p>  Mask token의 10% : 내 개는 크다 -&gt; 내 개는 크다</p></li></ul></li></ol><ol start="2"><li><p>Next Sentence Prediction</p><ul><li><p>BERT는 11개의 NLP Task에 대응하려고 했다.</p></li><li><p>하지만 11개의 Task 중에는 QA, QQ’similarity 등 문장에 대한 Task도 존재했다.</p></li><li><p>그래서 Next Sentence Prediction을 추가하여 범용성을 높이려고 했다.</p></li><li><p>예를 들면 다음과 같다.</p><p>  Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]<br>  Label = IsNext  </p><p>  Input = [CLS] the man went to [MASK] store [SEP] penguin [MASK] are flight<br>  Label = NotNext</p></li><li><p>첫번째 문장과 두번째 문장을 입력값으로 넣어 앞 뒤 문장이 연속되는 문장인지 분류하는 학습을 추가로 진행하였다.</p></li></ul></li></ol><h2 id="BERT-모델"><a href="#BERT-모델" class="headerlink" title="BERT 모델"></a>BERT 모델</h2><hr><ol><li><p>Model Input</p> <center><img src="/images/word_embedding/20190219_1706.png"></center><br><ul><li><p>입력값의 형태는 하나의 문장이 입력값이 될 수도 있고, 두개의 문장 쌍 ( 예를 들면 Q&amp;A )가 입력값이 될 수 있다.</p></li><li><p>e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)</p></li><li><p>Token Embedding : Glove, Word2vec, Fasttext와 같은 것을 사용해 Vector 값을 가져온다. </p></li><li><p>Segment Embedding : 단어가 첫번째 문장에 속하는지 두번째 문장에 속하는지 알려준다.<br>[ 0 0 0 0 0 1 1 1 1 ] 과 같이 표현하며 해당 Vector를 Token Embedding 차원수와 같게 맞추어 임베딩 해준다.</p></li><li><p>Positional Embedding : 각 단어가 첫번째인지 두번째인지를 의미하는 Embedding 값이다. 마찬가지로 Token Embedding의 차원수와 맟춰 임베딩 한 후 더해준다.<br>​</p></li></ul></li><li><p>Encoder Block</p> <center><img src="/images/word_embedding/20190219_1707.png"></center><br><ul><li><p>Transformer는 encode-decoder 구조로 decoder에서 loss function을 계산해서 trainin을 진행한다.</p></li><li><p>BERT에서는 Decoder 부분은 사용하지 않고 Encoder 부분만 사용을 한다. ( 붉은색 박스 부분 )</p></li><li><p>BERT는 N개의 encoder 블록을 사용한다. ( Base 모델은 12개, Large 모델은 24개로 구성 )</p></li><li><p>인코더 블록의 수가 많을수록 단어 사이의 복잡한 관계를 더 잘 포찰할 수 있다.<br>​</p></li></ul></li><li><p>Pre-training Procedure</p><ul><li><p>학습 데이터 : BooksCorpus(800M) + English Wikipedia(2,500M)위키 피디아의 경우 list, tables, header를 모두 제외하고 text만 사용</p></li><li><p>Batch size : 131,072 단어1Batch : 256 sequence x 512 words = 128,000 words 학습1Batch : 1024 sequence x 128 words = 131,072 words 학습</p></li><li><p>전체 step은 1,000,000번 으로 33억개의 단어 corpus에 대해서 대략 40 epoch 정도 학습</p></li><li><p>Dropout : 0.1 로 모든 레이어에 적용 </p></li><li><p>Activate function : gelu</p></li><li><p>BERT-Base : 12-layer, 768-hidden, 12-head</p></li><li><p>BERT-Large : 24-layer, 1024-hidden, 16-head</p></li></ul></li></ol><ol start="4"><li><p>Find-tuning Procedure</p> <center><img src="/images/word_embedding/20190219_1708.png"></center><br><ul><li><p>4개 정도 type의 언어 모델링 task를 만들고 output layer의 형태만 바꿔서 핵심 엔진인 BERT의 parameter 공유한다.</p></li><li><p>기본적으로 Transformer Encoder 부분의 Weight 는 Freeze 하고 추가적으로 하나의 Fully Connected Layer를 설계하여 우리가 원하는 Label 에 맞게 훈련하는 형태로 진행한다.</p></li></ul></li></ol><h2 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h2><hr><p>BERT 결과 종합</p><center><img src="/images/word_embedding/20190219_1709.png"></center><br><center><img src="/images/word_embedding/20190219_1710.png"></center><br><h2 id="공부할-때-참고했던-URL"><a href="#공부할-때-참고했던-URL" class="headerlink" title="공부할 때 참고했던 URL"></a>공부할 때 참고했던 URL</h2><hr><ul><li><p>[URL] : <a href="https://reniew.github.io/47/" target="_blank" rel="noopener">https://reniew.github.io/47/</a></p><ul><li>가장 Basic하게 설명되어 있는 페이지</li></ul></li><li><p>[URL] : <a href="https://www.slideshare.net/deepseaswjh/rnn-bert" target="_blank" rel="noopener">https://www.slideshare.net/deepseaswjh/rnn-bert</a></p><ul><li>짧게 짧게 흐름 위주로 설명을 했다.</li></ul></li><li><p>[URL] : <a href="http://www.modulabs.co.kr/DeepLAB_Paper/21074" target="_blank" rel="noopener">http://www.modulabs.co.kr/DeepLAB_Paper/21074</a></p><ul><li>모두의 연구소 자료</li><li>구체적으로 예시를 들어 설명해서 좋았다</li></ul></li><li><p>[URL] : <a href="http://hugrypiggykim.com/2018/12/09/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/" target="_blank" rel="noopener">http://hugrypiggykim.com/2018/12/09/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/</a></p><ul><li>transformer에 대한 설명이 자세하게 되어 있어서 너무 좋았다.</li><li>Attention 감 잡는데 굉장히 많은 도움이 되었다.</li></ul></li><li><p>[URL] : <a href="http://docs.likejazz.com/bert/" target="_blank" rel="noopener">http://docs.likejazz.com/bert/</a></p><ul><li>Basic 하게 설명되어 있다.</li></ul></li><li><p>[URL] : <a href="https://medium.com/ai-networkkr/%EC%B5%9C%EC%B2%A8%EB%8B%A8-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%86%94%EB%A3%A8%EC%85%98%EB%93%A4-1-%EA%B5%AC%EA%B8%80-bert-%EC%9D%B8%EA%B0%84%EB%B3%B4%EB%8B%A4-%EC%96%B8%EC%96%B4%EB%A5%BC-%EB%8D%94-%EC%9E%98-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-ai-%EB%AA%A8%EB%8D%B8-9704ebc016c4" target="_blank" rel="noopener">https://medium.com/ai-networkkr/%EC%B5%9C%EC%B2%A8%EB%8B%A8-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%86%94%EB%A3%A8%EC%85%98%EB%93%A4-1-%EA%B5%AC%EA%B8%80-bert-%EC%9D%B8%EA%B0%84%EB%B3%B4%EB%8B%A4-%EC%96%B8%EC%96%B4%EB%A5%BC-%EB%8D%94-%EC%9E%98-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-ai-%EB%AA%A8%EB%8D%B8-9704ebc016c4</a></p><ul><li>글쓴이의 해석이 들어가 있다. 도움이 되었다.</li></ul></li><li><p>[URL] : <a href="https://www.facebook.com/groups/TensorFlowKR/permalink/767590103582050/" target="_blank" rel="noopener">https://www.facebook.com/groups/TensorFlowKR/permalink/767590103582050/</a></p><ul><li>정말 깔끔 담백하게 정리되어 있다. </li><li>처음에 봤을 때는 잘 이해가 되지 않았지만 이해하고 보니까 이렇게 담백하게 표현할 수 있을까 싶다.</li></ul></li><li><p>[URL] : <a href="https://github.com/huggingface/pytorch-pretrained-BERT#Fine-tuning-with-BERT-running-the-examples" target="_blank" rel="noopener">https://github.com/huggingface/pytorch-pretrained-BERT#Fine-tuning-with-BERT-running-the-examples</a></p><ul><li>BERT를 사용해볼 수 있는 github</li><li>실제 돌려보니까 정말 이해가 잘 되었다.</li></ul></li><li><p>[URL] : <a href="http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/" target="_blank" rel="noopener">http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/</a></p><ul><li>코드와 함께 설명되어 있는 매우 희귀하고 좋은 자료</li></ul></li><li><p>[URL] : <a href="https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3" target="_blank" rel="noopener">https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3</a></p><ul><li>BERT와 Transformer를 잘 설명했다.</li></ul></li><li><p>[URL] : <a href="http://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html" target="_blank" rel="noopener">http://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html</a></p><ul><li>마찬가지로 코드와 함께 설명되어 있다.</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/23/word_embedding/bert/#disqus_thread</comments>
    </item>
    
    <item>
      <title>word2vec 실습</title>
      <link>https://vhrehfdl.github.io/2019/02/20/word_embedding/word2vec_example/</link>
      <guid>https://vhrehfdl.github.io/2019/02/20/word_embedding/word2vec_example/</guid>
      <pubDate>Wed, 20 Feb 2019 07:45:00 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;서론&quot;&gt;&lt;a href=&quot;#서론&quot; class=&quot;headerlink&quot; title=&quot;서론&quot;&gt;&lt;/a&gt;서론&lt;/h2&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Word2vec에 대한 설명은 word_embedding/word2vec에 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;한겨레 신문의 정치 기사 대략 1000개를 수집하였다.&lt;br&gt;위의 데이터를 사용해 word2vec을 구현한다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;코드는 github에 공개.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;[URL] : &lt;a href=&quot;https://github.com/vhrehfdl/Blog/tree/master/word_embedding&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/vhrehfdl/Blog/tree/master/word_embedding&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;총 3개의 python 파일로 구성되어 있다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="서론"><a href="#서론" class="headerlink" title="서론"></a>서론</h2><hr><ul><li><p>Word2vec에 대한 설명은 word_embedding/word2vec에 있다.</p></li><li><p>한겨레 신문의 정치 기사 대략 1000개를 수집하였다.<br>위의 데이터를 사용해 word2vec을 구현한다.</p></li><li><p>코드는 github에 공개.</p></li><li><p>[URL] : <a href="https://github.com/vhrehfdl/Blog/tree/master/word_embedding" target="_blank" rel="noopener">https://github.com/vhrehfdl/Blog/tree/master/word_embedding</a></p></li><li><p>총 3개의 python 파일로 구성되어 있다.</p></li></ul><a id="more"></a>​​<h2 id="구성"><a href="#구성" class="headerlink" title="구성"></a>구성</h2><hr><ol><li><p>make_corpus.py </p><ul><li><p>한겨레 기사는 여러 문장으로 단락이 구성되어져 있다.</p></li><li><p>그래서 문장 by 문장으로 잘라서 정리를 한 후 corpus.txt라는 파일을 만들어준다.</p></li></ul></li></ol><ol start="2"><li><p>make_token.py</p><ul><li><p>위에서 만든 corpus.txt라는 문장이 담겨있는 파일을 Token으로 변환시켜 corpus_token.txt라는 파일을 만든다.</p></li><li><p>영어는 NLTK를 활용하여 문장을 Token으로 나누어준다.</p></li><li><p>한국어는 형태소 분석기를 사용해 문장을 형태소 단위로 나누어 준다.</p></li><li><p>여기서는 Konlpy 라이브러리의 Komoran 모듈을 사용해 문장을 Token으로 변환시켜주었다.</p></li></ul></li><li><p>make_model.py</p><ul><li>Token으로 저장된 파일을 불러와 gensim의 word2vec에 입력한다.</li></ul></li></ol><h2 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h2><hr><ul><li><p>vector의 dimension은 100차원으로 정의하였다.</p></li><li><p>그래서 “서울”이라는 단어의 embedding은 아래와 같고<br>유사한 단어는 경기도, 강남구, 중구 등이 나왔다.</p></li></ul><center><img src="/images/word_embedding/20190227_1645.png"></center><br><ul><li>“김정일”이라는 단어를 넣으면 다음과 같은 결과가 나온다.</li></ul><center><img src="/images/word_embedding/20190227_1646.png"></center><br><p>​</p>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/20/word_embedding/word2vec_example/#disqus_thread</comments>
    </item>
    
    <item>
      <title>word2vec</title>
      <link>https://vhrehfdl.github.io/2019/02/19/word_embedding/word2vec/</link>
      <guid>https://vhrehfdl.github.io/2019/02/19/word_embedding/word2vec/</guid>
      <pubDate>Tue, 19 Feb 2019 04:36:00 GMT</pubDate>
      <description>
      
        &lt;h2 id=&quot;정리&quot;&gt;&lt;a href=&quot;#정리&quot; class=&quot;headerlink&quot; title=&quot;정리&quot;&gt;&lt;/a&gt;정리&lt;/h2&gt;&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;앞서 소개했던 BoW, TF-IDF는 Count Based Language Model에 속한다.  왜나하면 word를 vector로 표현할 때 단어의 빈도수를 특성으로 표현하기 때문이다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;이제부터 소개할 Language Model은 Neural Network Language Model이다. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NNLM에는 word2vec, FASTTEXT, Glove와 같은 방법들이 존재한다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h2><hr><ul><li><p>앞서 소개했던 BoW, TF-IDF는 Count Based Language Model에 속한다.  왜나하면 word를 vector로 표현할 때 단어의 빈도수를 특성으로 표현하기 때문이다.</p></li><li><p>이제부터 소개할 Language Model은 Neural Network Language Model이다. </p></li><li><p>NNLM에는 word2vec, FASTTEXT, Glove와 같은 방법들이 존재한다.</p></li></ul><a id="more"></a>​​<h2 id="Count-Based-Language-Model의-문제점"><a href="#Count-Based-Language-Model의-문제점" class="headerlink" title="Count Based Language Model의 문제점"></a>Count Based Language Model의 문제점</h2><hr><ul><li><p>Problem 1 : 하나의 단어를 표현하는데 큰 벡터가 필요하다.</p><ul><li><p>기존의 원 핫 인코딩에서 하나의 Column은 사전에 등록된 단어를 의미했다.</p><p>  강아지 : [ 1 0 0 ]<br>  멍멍이 : [ 0 1 0 ]<br>  고양이 : [ 0 0 1 ]</p></li><li><p>만약 사전에 30,000개의 단어가 들어있다면 하나의 단어를 표현하기 위해서는 30,000 차원이 필요하다. ( Count Based LM은 듬성 듬성한 Sparse Vector의 모습을 가지게 된다. )</p></li><li><p>큰 차원의 벡터는 계산복잡성이 크고 메모리 문제가 존재한다. 그리고 실제 위의 벡터에서 필요한 값은 1이 들어간 하나의 요소 뿐이고 나머지는 불필요한 0들만 포함되어 있다.</p></li></ul></li><li><p>Problem2 : 단어와의 관련성을 파악할 수 없다.</p><ul><li><p>예를 들어 “강아지”와 멍멍이”가 유사하다는 것을 Count Based LM에서는 알 수 없었다. 저런 형태로는 관계를 전혀 파악할 수가 없다.</p><p>  강아지 : [ 1 0 0 ]<br>  멍멍이 : [ 0 1 0 ]</p></li><li><p>또한 위의 Vector를 내적하면 0이 된다. 길이가 1인 두 벡터의 내적은 두 벡터 사이의 각도가 되므로 두 벡터가 직교한다는 의미가 된다.</p></li><li><p>이를 확장해서 생각하면 원 핫 인코딩은 서로 독립한다는 것을 알 수 있다.  </p></li><li><p>즉, 모든 단어들이 서로 영향을 미치지 않는 다는 것을 의미한다.<br>​</p></li></ul></li></ul><h2 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed Representation"></a>Distributed Representation</h2><hr><ul><li><p>위의 문제들을 해결하기 위해 Distributed Representation(분산 표상)을 사용한다.  ​</p></li><li><p>( 그림1 )을 보면 총 9개의 도형이 있다.</p><p>  초록색 해 / 초록색 달 / 초록색 구름<br>  파란색 해 / 파란색 달 / 파란색 구름<br>  주황색 해 / 주황색 달 / 주황색 구름</p><p>  만약 파란색 해를 One hot encoding으로 표현한다면 [ 0 0 0 1 0 0 0 0 0 ] 이 될 것이다.</p>  <center><img src="/images/word_embedding/20190219_1336.png" alt="(그림1) 출처 : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/"></center><br></li><li><p>위의 9차원 벡터는 2차원 벡터로도 표현 가능하다.  각 도형의 속성인 colors와 shapes를 조합하여 9개의 도형을 표현할 수 있기 때문이다.  </p></li><li><p>분산표상은 데이터의 차원수를 줄여주기도 하지만 개체간 유사성을 비교할 수도 있게 만든다. 의미가 유사한 단어는 벡터 공간에서 가깝게, 반대의 경우에는 멀게 배치하는 것이 분산표상의 목표이다.</p></li><li><p>Word2vec에서 단어 vector는 “강아지” = [ 0.2, 1.8, 2.8 … , -7.2 ]와 같이 표현된다. </p></li><li><p>이전의 Count-Based LM에서 vector의 column은 사전에 등록된 단어를 의미했다.</p></li><li><p>그렇다면 Word2vec에서 vector의 column은 위에서 언급한 colors, shapes와 같은 특성을 의미하는 것일까?</p></li><li><p>정답은 아니다. 그렇게 딱 딱 떨어지는 것이 아닌 colors와 shapes의 특성이 혼합된 것을 의미한다. </p></li><li><p>즉 “강아지”라는 단어 vector에서 0.2 가 동물, 1.8이 종류를 나타내는 것이 아니다. 여러 특성이 섞여서 수치로 표현 되는 것이다.    </p></li><li><p>“강아지” = [ 0.2, 1.8, 2.8 … , -7.2 ]와 같이 vector의 값이 dense(빽빽하게) 있기 때문에 dense vector라고도 한다.</p></li></ul><h2 id="Neural-Network-Language-Model"><a href="#Neural-Network-Language-Model" class="headerlink" title="Neural Network Language Model"></a>Neural Network Language Model</h2><hr><ul><li>위에서 설명한 분산 표상을 가지는 Vector를 만들기 위해서 Shallow Neural Net 학습을 진행한다.</li></ul><center><img src="/images/word_embedding/20190219_1337.png" alt="(그림2) 출처 : https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa"></center><br>  <ul><li><p>Input Layer에는 One-Hot Encoding의 값이 들어간다.<br>강아지 = [ 1 0 0 ] 의 값이 들어가는 것이다.<br>x1 = 1 / x2 = 0 / x3 = 0</p></li><li><p>Hidden Layer에서 Weight와 Bias의 값이 존재한다.</p></li><li><p>Hidden Layer의 노드 개수는 사용자가 정의한다. ( Hidden Layer의 노드 개수가 embedding dimention을 의미한다. )</p></li><li><p>보통 128 / 256 / 512와 같이 2의 배수를 사용해서 정의한다. ( 딥러닝 학습 연산과 관련이 있다.)</p></li><li><p>Output Layer에서는 Input Layer와 노드의 개수가 같다.<br>(Input Layer와 Output Layer 노드 개수는 One-hot Encoding의 dictionary 개수와 같다.)<br>출력값은 [ 0 1 0 ] 을 가지게 된다.<br>y1 = 0 / y2 = 1 / y3 = 0<br>멍멍이 = [ 0 1 0 ]</p></li><li><p>NNLM의 학습은 다음 단어가 무엇인지를 예측하는 방식으로 이루어진다.  </p></li><li><p>“The fat cat sat on the mat” 이라는 문장이 있다.<br>Input에는 The = [ 1 0 0 0 0 0 0 ] 이 들어가고 hidden layer를 거쳐서 predict 한 값이 [ 0 1 0 0 0 0 0 ]이 되도록 학습을 진행하다는 것이다.   </p></li><li><p>이렇게 학습을 진행하면 단어간의 의미적인 관계가 담겨있는 weight vector가 생성된다는 것이 NNLM의 기본개념이다.</p></li><li><p>CBOW와 Skip-gram을 설명하면서 더 자세하게 설명하겠다.</p></li></ul><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><hr><ul><li><p>NNLM의 학습 방법은 CBOW와 Skip-Gram 방법이 존재한다.</p></li><li><p>CBOW는 주변 단어를 사용해 중간 단어를 예측하는 방법을 의미한다. </p></li></ul><p>​<center><img src="/images/word_embedding/20190219_1338.png" alt="(그림3) 출처 : https://wikidocs.net/22660"></center><br>  </p><ul><li><p>(그림3)을 보면 중심 단어를 기준으로 앞뒤 2개의 단어를 학습데이터로 사용을 한다.</p></li><li><p>주변 단어를 Input에 넣고 Hidden Layer를 거쳐서 Output Layer에서 나온 값이 중심 단어가 되도록 학습시키는 것이다.</p></li></ul><p>​<center><img src="/images/word_embedding/20190219_1339.png" alt="(그림4) 출처 : https://wikidocs.net/22660"></center><br>  </p><ul><li><p>(그림4)는 (그림3)을 이해하기 쉽게 도식화한 이미지이다.</p></li><li><p>sat이라는 중심단어를 예측하기 위해 fat, cat, on, the와 같은 주변 단어를 입력하였다.</p></li></ul><p>​<center><img src="/images/word_embedding/20190219_1340.png" alt="(그림5) https://wikidocs.net/22660"></center><br>  </p><ul><li>(그림5)를 보면 알 수 있듯이 4개의 벡터 값의 평균을 구하여 해당 Vector 값을 Word Vector로 사용한다.</li></ul><p>​<br>​</p><h2 id="Skip-Gram"><a href="#Skip-Gram" class="headerlink" title="Skip-Gram"></a>Skip-Gram</h2><hr><ul><li><p>Skip-gram은 CBOW와 반대로 중심단어를 사용해 주변단어를 예측하는 것이다.<br>보통 CBOW보다 Skip-gram을 많이 사용한다고 하는데 이유는 다음과 같다.</p></li><li><p>CBOW에서는 주변단어를 4개를 사용해 predict를 하고 한 번만 역전파를 한다.<br>하지만 skip-gram에서는 중심단어 1개를 사용해 predict를 하고 네 번 역전파를 진행한다.</p></li><li><p>즉, skip-gram이 역전파하는 회수가 CBOW보다 더 많기 때문에 학습이 잘 된다고 한다.</p></li></ul><p>​<center><img src="/images/word_embedding/20190219_1340.png" alt="(그림6) 출처 : https://wikidocs.net/22660"></center><br><br>​<br>​</p><h2 id="정리-1"><a href="#정리-1" class="headerlink" title="정리"></a>정리</h2><hr><ul><li>word2vec을 이용해 단어의 vector를 구한다는 것은 NNLM의 신경망에서 학습된 가중치 벡터를 가져오는 것이다. </li></ul><p>​<center><img src="/images/word_embedding/20190219_1340.png" alt="(그림7) 출처 : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/"></center><br> </p><ul><li><p>( 그림7 ) 과 같이 좌측 행렬은 단어를 의미하는 행렬이고 우측 행렬은 weight vector이다.</p></li><li><p>“강아지”라는 단어는 [ 10 12 19 ] 라는 벡터로 표현되는 것이다.<br>이것은 weight vector의 행이 단어의 벡터를 의미한다고 볼 수 있다.</p></li><li><p>왜냐하면 [ 0 0 0 1 0 ]에서 보는 것처럼 0과 곱해지면서 1이 포함된 weight vector의 행만 출력이 되기 때문이다.<br>(말로 잘 설명이 안되네요… 행렬의 곱셈을 할 줄 아는 사람이라면 충분히 이해할 것이라 생각합니다.)</p></li><li><p>그렇다면 weight vector는 어디서 가져올까?</p></li><li><p>wiki-docs 와 같은 데이터를 Pre-trained한 Weight vector 값을 가져올 수 있다.</p></li><li><p>아니면 자신의 목적에 맞게 학습을 시켜 Weight vector를 생성하고 그 vector 값을 가져와 사용할 수 있다.</p></li><li><p>wiki-docs에서 사전에 학습한 weight vector 값을 가져온다고 가정해보자.<br>wiki-docs가 많은 단어를 가지고 있기는 하지만 내가 사용하는 데이터의 단어를 가지고 있지 않은 경우도있을 것이다.</p></li><li><p>그런 경우에는 해당 단어의 vector 값을 random 하게 부여하기도 하고 값을 크게 부여하기도 한다.<br>자기가 설계하기 나름이다.</p></li><li><p>기존의 Count Based LM에서는 사전의 단어 개수가 차원수 였다면 NNLM에서는 사용자가embedding_dim을 몇으로 정의하는가에 따라 단어의 차원수가 정해진다. ( 처음에는 이게 이해가 안 되서 고생을 했다. )</p></li></ul><p>​</p><h2 id="공부할-때-참고했던-URL"><a href="#공부할-때-참고했던-URL" class="headerlink" title="공부할 때 참고했던 URL"></a>공부할 때 참고했던 URL</h2><hr><ul><li><p>[URL] : <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/" target="_blank" rel="noopener">https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/</a></p><ul><li>word2vec 학습 방식  </li><li>개괄적으로 잘 설명해두었다.<br>​</li></ul></li><li><p>[URL] : <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/" target="_blank" rel="noopener">https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/</a></p><ul><li>Distributed Representation 잘 설명</li><li>Neural Network Language Model을 설명</li></ul></li><li><p>[URL] : <a href="https://wikidocs.net/22660" target="_blank" rel="noopener">https://wikidocs.net/22660</a></p><ul><li>NNLM의 학습방법에 대해 매우 자세히 설명되어 있다.</li></ul></li><li><p>[URL] : <a href="https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html" target="_blank" rel="noopener">https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html</a></p><ul><li>Count-Based LM 부터 NNLM의 방법까지 잘 설명되어 있다.</li></ul></li><li><p>[URL] : <a href="https://lovit.github.io/nlp/2018/04/05/space_odyssey_of_word2vec/" target="_blank" rel="noopener">https://lovit.github.io/nlp/2018/04/05/space_odyssey_of_word2vec/</a></p><ul><li>코드와 구체적인 예시등이 설명되어 있다.</li></ul></li><li><p>[URL] : <a href="https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/word2vec/" target="_blank" rel="noopener">https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/word2vec/</a></p><ul><li>텐서플로우 코리아 word2vec 문서</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://vhrehfdl.github.io/2019/02/19/word_embedding/word2vec/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
