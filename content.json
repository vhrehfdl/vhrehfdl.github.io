{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","link":"/2018/01/15/hello-world/"},{"title":"GCS API 403 caller has no permission","text":"에러 : 403 caller has no error.GCS 사용하다가 마주친 에러 Solution 1 : 결제 에러 관련 bucket을 새로 만들어본다. 조회 명령어 gsutil ls gs://my-awesome-bucket bucket 새로 생성 gsutil mb -l us-east1 gs://my-awesome-bucket/ Solution 2 : 권한 에러 관련 공개로 돌려버리면 된다. all users를 추가하면 된다.","link":"/2019/02/14/error_set/google_cloud_storage_error/"},{"title":"Google Speech API 아무것도 출력이 되지 않는 에러","text":"원인아래 코드의 sample_rate_hertz가 일치하지 않아서 그렇다.ffmpeg를 사용해 동영상 확장자를 변환할 때 맞춰주어야 한다. 해결방법 ffmpeg로 변환을 시킨다. sample_rate_hertz가 16000으로 설정했기 때문에 변활시킬 때 16000으로 바꿔주어야 한다. ffmpeg -i audio_file.mp4 -acodec pcm_s16le -ac 1 -ar 16000 audio_file.wav 예제 코드 def transcribe_file(speech_file): \"\"\"Transcribe the given audio file.\"\"\" from google.cloud import speech from google.cloud.speech import enums from google.cloud.speech import types client = speech.SpeechClient() with io.open(speech_file, 'rb') as audio_file: content = audio_file.read() audio = types.RecognitionAudio(content=content) config = types.RecognitionConfig( encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16, sample_rate_hertz=16000, language_code='ko-KR') response = client.recognize(config, audio) print(\"test2\") # Each result is for a consecutive portion of the audio. Iterate through # them to get the transcripts for the entire audio file. for result in response.results: print(\"test3\") # The first alternative is the most likely one for this portion. print(u'Transcript: {}'.format(result.alternatives[0].transcript))speech_file = \"audio_file.wav\"transcribe_file(speech_file)","link":"/2019/02/25/error_set/google_speech_api_error/"},{"title":"python encoding error","text":"에러 메세지 : UnicodeDecodeError: ‘utf-8’ codec can’t decode byte 0xff in position 0: invalid 해결 방법 : encoding 부분을 utf-8에서 utf-16으로 바꿔주니까 된다. ( python3 기준 ) 예시 코드 # -*- coding: utf-8 -*-:import csvf = open('1_50000.csv', 'r', encoding='utf-16')rdr = csv.reader(f)for line in rdr: print(line)f.close()","link":"/2019/03/06/error_set/python_encoding_error/"},{"title":"pandas No module named 'pandas.core.internals.managers'","text":"Error Message : ModuleNotFoundError: No module named ‘pandas.core.internals.managers’; ‘pandas.core.internals’ is no ​ 발생 원인 A컴퓨터에서 잘 작동했던 파이썬 코드를 B컴퓨터에서 실행시키니 저 error가 계속 발생했다.처음에는 pandas 환경과 Python 버전 환경이 달라서 발생한 줄 알았다.그래서 환경을 똑같이 맞추어줬는데도 계속 에러가 발생했다.일단 에러 발생 부분을 보니 pickle.load() 하는 부분에서 에러가 발생했다. 해결 방법 결국 해결한 방법은 A 컴퓨터에서 pickle 파일 만들 때 pandas.DataFrame으로 넣지 않고 pandas.to_dict()로 바꿔서 파일을 만들었다.B 컴퓨터에서 pickle 파일 불러오고 dict 파일을 pandas.DataFrame으로 넣어서 해결했다.pickle 파일에 데이터 넣을 때 함부로 넣으면 안 되나 보다.","link":"/2019/04/07/error_set/pandas_error/"},{"title":"pycharm java_home error","text":"에러 : Pycharm에 JAVA_HOME이 설정되어 있지 않다. 에러 원인 : Jpype가 설치되어 있지 않았고 JAVA_HOME이 설정 되어있지 않아서 에러가 발생했다. 해결 방법 JPype1-py3을 pycharm setting에 들어가서 설치해준다.위에 것을 설치해도 에러가 발생한다면 JAVA_HOME이 설정되어 있지 않아서이다. 시스템 변수에 JAVA_HOME을 추가해주었더니 실행이 되었다. 참고 URLhttps://mainia.tistory.com/5667","link":"/2019/02/01/error_set/pycharm_java_home_error/"},{"title":"php exec error","text":"에러 PHP에서 Python 스크립트 파일을 실행시켰다. Python 스크립트 파일에는 pytube를 이용해 영상을 다운받는 코드가 있었다. 하지만 download 가 되지 않는 문제가 있었다. 에러 원인 PHP에서 exec를 실행하면 www-data 일반 유저의 권한으로 해당 스크립트 파일을 실행시킨다.일반 유저의 권한으로는 파일 읽기 쓰기가 되지 않기 때문에 에러가 났었다. 해결방법 해당 파일들의 권한을 chmod 777이거나 쓰기 가능할 정도로 변경해주면 해결 가능하다.우선적으로 sudo 권한 아닐 때 실행시켜보면 에러 잡을 수 있다. 생각해봐야 할 것 근데 일반 서비스에서는 이렇게 함부로 권한 남발하면 안되는데….고민을 해봐야 할 문제이다. 참고 URL http://blog.naver.com/PostView.nhn?blogId=kkyy3402&amp;logNo=221284921252&amp;categoryNo=0&amp;parentCategoryNo=0&amp;viewDate=¤tPage=1&amp;postListTopCurrentPage=1&amp;from=postView​ http://blog.naver.com/PostView.nhn?blogId=anducher&amp;logNo=130033390734&amp;widgetTypeCall=true","link":"/2019/02/07/error_set/php_exec_error/"},{"title":"Scrapy AttributeError module 'lib' has no attribute 'Cryptography_HAS_SSL_ST","text":"Error Message : AttributeError module ‘lib’ has no attribute ‘Cryptography_HAS_SSL_ST 원인 Scrapy 설치하고 실행하려는데 에러가 발생했었다.찾아보니까 pyOpenSSL 문제인 것 같다.그냥 install 하나 해주니까 해결되었다. 해결방법 pip3 install pyOpenSSL==0.15.1","link":"/2019/02/26/error_set/scrapy_error/"},{"title":"Loaded runtime CuDNN library 7101","text":"에러 메세지 : Loaded runtime CuDNN library: 7101 (compatibility version 7100) but source was compiled with 7003 (c 에러 원인 : tensorflow-gpu가 돌아가는 환경설정이 CUDA Toolkit 버전과 cuDNN 버전이 호환되지가 않아서 발생한 문제이다. 해결 방법 : Toolkit은 9.0 cuDNN은 7.0.5로 호환해주니 해결되었다.","link":"/2019/04/07/error_set/tensorflow_gpu_error/"},{"title":"Tensorflow RuntimeError Missing implementation that supports loader","text":"에러 메세지 : RuntimeError: Missing implementation that supports: loader(*(‘/tmp/tfhub_modules/mobilenet_module’,) 해결 방법 : loader 뒤에 뜨는 에러메세지 경로에 들어가서 tf_hub 파일을 지워주고 다시 실행시키니 작동하였다.이게 갑자기 어느 순간에 에러가 발생하는데… 왜 발생하는지는 모르겠다.그래도 해결방법이 단순하다. 참고 URLhttps://stackoverflow.com/questions/54029556/how-to-fix-runtimeerror-missing-implementation-that-supports-loader-when-cal","link":"/2019/04/04/error_set/tensorflow_hub_error-1/"},{"title":"Tensorflow ImportError cannot import name 'regex_replace'","text":"해결 방법 : ELMo 사용하려고 Tensorflow Hub를 설치하고 실행시켜보니 위에 error 발생!처음에 0.3 버전으로 설치했는데…혹시나 해서 버전 0.2로 낮춰서 해보니 실행이 되었다…","link":"/2019/03/27/error_set/tensorflow_hub_error/"},{"title":"ImportError dll load failed 지정된 모듈을 찾을 수 없습니다","text":"Error Message : ImportError: dll load failed: 지정된 모듈을 찾을 수 없습니다 문제 상황 import torch는 문제가 없이 잘 되었는데…import torchvision에서 에러가 터졌다.지정된 모듈을 찾을 수 없다는 에러가 터졌다. 이게 어디서 자주 본 에러여서 기억을 더음어 보니…tensorflow gpu 할 때 본 에러였다. 해결 방법 파일 설치해보기 Intel-openmp visual studio 2017 재배포파일 Intel-openmp는 anaconda 환경에 설치하면 된다.이 방법으로도 해결이 되지 않으면 2번 방법을 적용해보자. torchvision 버전 낮춰보기 위에 방법으로 해결되지 않는다면 torchvision 버전을 0.4버전에서 0.2버전으로 낮춰보자. 나는 이렇게 해서 해결했다. 내 cuda 버전과 torchvision 버전이 호환되지 않아서 발생한 문제인 것 같다. 참고 URL [URL] : https://mclearninglab.tistory.com/30","link":"/2019/09/11/error_set/pytorch_error/"},{"title":"tensorflow version error","text":"에러 메세지 : TypeError: Expected int32, got list containing Tensors of type _Message instead. 에러 원인 : Tensorflow가 1.x 대로 업그레이드 되면서 tf.concat의 input이 달라져서 생기는 문제이다. 이미지와 같이 수정해준다. 해결 방법 : parameter 위치만 바꿔주면 된다.","link":"/2019/01/31/error_set/tensorflow_version_error-2/"},{"title":"tensorflow version error","text":"에러 메세지 : Only call softmax_cross_entropy_with_logits with named argument 에러 원인 : 텐서플로우가 1.x 대로 업그레이드 되면서 생긴 문제이다.함수 사용할 때 이미지와 같이 parameter를 구체적으로 표시해주어야 한다. 해결 방법 : labels 와 logits를 붙여주어야 한다.","link":"/2019/01/31/error_set/tensorflow_version_error-1/"},{"title":"tensorflow version error","text":"에러 메세지 : batch() got an unexpected keyword argument ‘drop_remainder’ 에러 원인 : Tensorflow 버전이 업그레이드 되면서 run_classifier.py의 drop_remainder 파라미터가 없어졌다. 해결 방법 : runclassfier.py 파일에서 drop_remainder 코드를 제거해줘야 한다.","link":"/2019/04/08/error_set/tensorflow_version_error/"},{"title":"Ubuntu E Problem executing scripts","text":"Error : Ubuntu E: Problem executing scripts APT우분투에서 apt-get 사용하다 발생한 error Solution sudo apt-get remove libappstream3 쉽게 해결된다.","link":"/2019/02/24/error_set/ubuntu_error/"},{"title":"Bag of Words","text":"소개 Bag of Words란 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법이다. ​ 예시 세개의 문장이 있다고 가정하고 위의 문장을 Bag of Words로 표현하면 다음과 같다. sentence_list = [‘오늘은 즐거운 하루입니다’, ‘오늘은 비가 올 것 같은데 내일은 비가 안 왔으면 좋겠네요.’, ‘내일도 비가 올 것 같아요’ ] 위 세 문장의 전체 단어수는 14개이다. ( 오늘은 / 즐거운 / 하루입니다 / 비가 / 올 / 것 / 같은데 / 내일은 / 안 / 왔으면 / 좋겠네요 / 내일도 / 것 /같아요 ) 세 문장의 단어 사전은 아래와 같다. [ 오늘은 : 1 / 즐거운 : 2 / 하루입니다 : 3 / 비가 : 4 / 올 : 5 / 것 : 6 / 같은데 : 7 / 내일은 : 8 / 안 : 9 / 왔으면 : 10 / 좋겠네요 : 11 / 내일도 : 12 / 것 : 13 / 같아요 : 14 ] 위의 세 문장을 Vector로 표현하면 다음과 같다. 1 1 1 0 0 0 0 0 0 0 0 0 0 01 0 0 2 1 1 1 1 1 1 1 0 0 00 0 0 1 1 0 0 0 0 0 0 1 1 1 한 문장내에서 중복되는 단어는 카운트하여 표시해준다. 생각 사실 BoW를 공부하면서 가장 이해가 안 되었던 것은 One hot encoding과의 차이점이였다. 빈도 수 카운트 차원 수 첫번째, one hot encoding은 중복 단어를 카운팅하지 않는다.즉, 0과 1로만 표현한다.하지만 BoW는 중복되는 단어를 카운트하기 때문에 빈도수를 측정할 수 있다. 두번째, one hot encoding으로 문장을 표현하면 n x m 과 같이 2차원으로 표현된다.( n은 문장에 있는 단어 개수 / m은 사전에 있는 단어의 개수 )BoW는 1 x m 과 같이 1차원으로 표현된다.( m은 사전에 있는 단어의 개수 이다. ) ​ ​","link":"/2019/01/31/word_embedding/bag_of_words/"},{"title":"One Hot Encoding","text":"소개 One Hot Encoding이란 벡터 표현 방법이다.One Hot Encoding을 이용해 표현한 벡터를 One Hot Vector라고 한다. ​ 예시 아래 문장에 One Hot Encoding을 적용하면 다음과 같은 벡터로 표현가능하다. sentence_list = [‘오늘은 즐거운 하루입니다’, ‘오늘은 비가 올 것 같군요’, ‘내일도 비가 올 것 같아요’ ] 세 문장의 전체 단어수는 9개이다. ( 오늘은 / 즐거운 / 하루입니다 / 비가 / 올 / 것 / 같군요 / 내일도 / 같아요 )​ 세 문장의 단어로 사전을 만들면 아래와 같이 된다. [ 오늘은 : 1 / 즐거운 : 2 / 하루입니다 : 3 / 비가 : 4 / 올 : 5 / 것 : 6 / 같군요 : 7 / 내일도 : 8 / 같아요 : 9 ]​ ‘오늘은 즐거운 하루입니다’ 문장을 One Hot Vector로 만들면 다음과 같이 표현된다. ( Word 단위로 자른다고 가정 ) 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 ‘오늘은 비가 올 것 같군요’ 문장을 One Hot Vector로 만들면 다음과 같이 표현된다. 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 위와 같이 표현하고 싶은 단어의 index 위치에 1을 부여하고, 다른 단어의 인덱스의 위치에는 0을 부여하는 방법으로 표현한다. 생각 One Hot Encoding은 단어의 수가 많아지면 Vector의 차원이 계속해서 증가하는 문제가 발생한다. 그리고 단어 사이의 유사성과 관계 등을 파악할 수 없다는 문제점이 있다. ex) 멍멍이의 vector는 [ 0 1 0 0 0 … 0 ] / 강아지의 vector는 [ 0 0 1 0 0 … 0 ] 으로 표현되어 있다고 가정해보자. 위의 vector만으로는 두 단어가 의미적으로 유사한지 알 수 없다.","link":"/2019/01/31/word_embedding/one_hot_encoding/"},{"title":"TF-IDF","text":"tf idf 설명. ​ 소개 TF-IDF란 단어의 빈도와 역문서 빈도를 사용하여 단어들마다 가중치를 부여하는 방법이다. TF ( Term Frequency ) : 특정 문서 d에서 특정 단어 t의 등장 회수ex) “과자”라는 단어가 문서3에서 100번 등장했다. DF( Document Frequency ) : 특정 단어 t가 등장한 문서의 수ex) “과자”라는 단어가 문서2와 문서3에서 언급되었다.이럴 경우 DF는 2이다. 몇 개의 문서에서 언급되었는지를 알아야 한다.​ IDF ( Inverse Document Frequency ) : DF(t)에 반비례하는 수분모에 1을 추가해주는 이유는 df가 0일 경우에 분모가 0이 되기 때문이다.​분자의 n은 전체 문서의 개수를 의미한다. ​ex) “과자”라는 단어가 문서1에서 0번, 문서2에서 100번, 문서3에서 200번 언급되었다고 가정하자. log(분자 : 3 / 분모 : 1+2) 가 된다. 여러 문서에서 자주 사용하는 단어일수록 값이 0에 가까워지기 때문에 TF x IDF한 값이 작아지게 된다. 예시 전체 소스코드는 github에 공개하였다. [code] : https://github.com/vhrehfdl/Blog/blob/master/machine_learning/TF-IDF.py sentence_list = [‘오늘은 즐거운 하루입니다’, ‘오늘은 비가 올 것 같은데 내일은 비가 안 왔으면 좋겠네요.’, ‘내일도 비가 올 것 같아요’ ] BoW 앞에서 진행 했던 BoW로 위의 sentence_list를 표현하면 다음과 같다. 단어수가 한 글자인 것은 제외해서 vector를 구성하였다. TF-IDF ver.1 가장 basic한 형태의 TF-IDF이다. 위의 BoW에서 단어의 가중치를 Vector로 표현을 하였다. TF-IDF ver.2 한 글자인 단어들도 Vector에 포함하였다. TF-IDF ver.3 위의 TF-IDF에서 ngram_range=(2, 3)를 추가하였다. 단어를 2글자 or 3글자 단위로 묶어 처리하였다. TF-IDF ver.4 ngram_range=(2, 3)를 추가하고 analyzer=’char’로 변경하였다. char로 변경하였기 때문에 단어가 아닌 글자가 기준이 된다. 생각 TF-IDF 의 개념과 사용방법은 생각보다 쉽다. 문제는 이러한 단어의 가중치들이 알고리즘에 어떻게 사용되는지를 이해하는 것이 중요한 것 같다. 뭔가 단어에다가 가중치를 준다고 하면 더 좋을 것 같아보인다. 하지만 실제 성능이 향상하는지 확인해봐야 알 것 같다. ​​ ​","link":"/2019/02/08/word_embedding/tf_idf/"},{"title":"BERT","text":"BERT 소개 Google 에서 만든 Word Embedding 기법 ( 2018. 10. 11 논문 공개 ) NLP 11개 Task에 SOTA(State of the Arts)를 기록했으며, SQuAD v1.1에서는 인간보다 더 높은 정확도를 보여 주목을 받고 있다. 최근까지 GLUE NLP Task 에서 1등을 차지했었다. ( 그러나 MT-DNN에 1등을 뺏겼다. ) Pre-trained 기반 딥러닝 언어 모델 BERT 개발자들의 접근방식 : (1) 범용 솔루션을 (2) 스케일러블 한 형태로 구현해서 (3) 많은 머신리소스로 훈련해서 성능을 높인다 BERT는 Contextual Embedding 방법에 속한다. (Contextualised Word Embedding은 단어마다 벡터가 고정되어 있지 않고 문장마다 단어의 Vector가 달라지는 Embedding 방법을 뜻한다 대표적으로 ELMo, GPT, BERT가 있다.) ​​ Static Word Embedding 문제점 Static Word Embedding은 단어마다 벡터가 고정되어 있는 방법을 뜻하면 대표적으로 Word2vec, Fasttext, Glove이 존재한다. Problem : 단어의 Vector가 모든 문맥에서 동일하다. 에를 들어 “배를 타고 떠났다”와 “맛있는 배를 먹었다”라는 문장에서 “배를”은 같은 벡터 값을 가진다. “배를” 이란 단어는 One-hot Encoding 방식으로 표현하면 [ 0 0 0 1 0 ]의 값을 가진다. 첫번째 문장에서도 [ 0 0 0 1 0 ]을 가지고 두번째 문장에서도 [ 0 0 0 1 0 ]을 가진다.모든 문장에서 고정된 One-hot Encoding 값을 가진다! 모든 단어들이 고정된 One-hot Encoding을 가지는 상태에서 Weight Vector를 곱하면 모든 문장에서 똑같은 Vector 값을 가지게 된다. ( 그림1을 보고 충분히 생각해보면 왜 고정된 Embedding Vector 값이 나오는지 알 수 있을 것이다. ) 그리고 Static Word Embedding 에서는 Shallow Neural Net을 사용해서 학습을 진행했었다.Shallow Neural Net은 LSTM이나 RNN과 같이 순환 신경망 계열이 아니기 때문에 문맥 정보가 반영되지 않고 학습이 진행된다. Contextual Language Model Semi-Supervised Sequence Learning, Google, 2015 Shallow Neural Net이 아닌 RNN, LSTM, GRU와 같은 순환신경망은 이전 정보를 반영해서 학습을 진행한다. ELMo : Deep Contextual Word Embeddings, AI2&amp;University of Washington, 2017 문장의 좌측에서 우측으로 우측에서 좌측으로 따로 따로 학습한다. 왜냐하면 문맥정보를 조금더 많이 반영하기 위해서 전-&gt;후 뿐만 아니라 후-&gt;전으로도 학습을 진행한다. GPT : Improving Language Understanding by Generative Pre-Training, OpenAI, 2018 LSTM이 아닌 Transformer를 사용해 학습을 진행한다.​ BERT BERT는 GPT와 같이 Transformer를 이용하여 ELMo와 같이 양방향으로 학습을 진행한다. 사람도 언어를 이해할 때 양방향으로 이해하기 때문에 양방향으로 학습을 진행하면 성능이 좋아질 것이라 가정. ( BERT의 가장 큰 특징이라고 할 수 있다. ) ELMo와는 다르다. ELMo는 단방향으로 좌측 우측 각 각 학습하는 것이고 BERT는 양방향으로 동시에 학습을 진행하는 것이다. 하지만 양방향 학습을 하게 되면 단어가 자기 자신을 참조하는 문제가 발생한다. “However, it is not possible to train bidirectional models by simply conditioning each word on its previous and next words, since this would allow the word that’s being predicted to indirectly “see itself” in a multi-layer model.” 예를 들어 “배를 타고 떠났다.”라는 문장이 있고 단방향 학습을 진행한다고 가정해보자.“타고” 라는 단어를 학습할 때 “배를”이라는 단어만을 사용해서 학습을 진행한다.자기 자신을 참조할 일이 전혀 없다. (그림5)를 보게 되면 빨간 박스 부분 학습에 영향을 미치는 Layer는 파란색 박스 밖에 없다.​ 하지만 양방향 학습 같은 경우에는 영향을 미친다. (그림6) 을 보면 빨간 박스에 직접적으로 영향을 미치는 것들은 파란색 박스이다. 하지만 파란색 박스에 초록색 박스 또한 영향을 미치고 있다. “타고”라는 단어 또한 직접적이지는 않지만 간적적으로 학습에 영향을 미친다는 것이다. BERT 특징 Masked LM (그림7)과 같이 자기 자신을 참조하는 문제를 해결하기 위해 MASK를 씌워 학습을 진행한다.이러한 방법이 오래전부터 문맥을 파악하는데 제시되었던 방법이라고 한다. Masked LM을 사용하게 되면 또 다른 문제가 발생하게 된다. 실제 Fine-tuning과정에 학습 데이터에는 Mask toeknd이 존재하지 않는다는 것이다. 문장에 MASK를 씌우는 거는 Pre-trained 학습에서만 하지 실제 fine-tuning 할 때는 씌우지 않기 때문에 간극이 발생할 것이라 생각했다고 한다. 그에 대한 해결책으로 Mask token의 80%는 mask로 10%는 random word로 10%는 unchanged word로 넣어준다. ( 즉, 일부러 Noise를 넣음으로서 너무 Deep 한 모델에서 발생할 수 있는 Over-fitting문제를 회피하도록 의도하였다 ) 예를 들면 아래와 같이 진행했다. Mask token의 80% : 내 개는 크다 -&gt; 내 개는 [MASK] Mask token의 10% : 내 개는 크다 -&gt; 내 개는 사과 Mask token의 10% : 내 개는 크다 -&gt; 내 개는 크다 Next Sentence Prediction BERT는 11개의 NLP Task에 대응하려고 했다. 하지만 11개의 Task 중에는 QA, QQ’similarity 등 문장에 대한 Task도 존재했다. 그래서 Next Sentence Prediction을 추가하여 범용성을 높이려고 했다. 예를 들면 다음과 같다. Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man went to [MASK] store [SEP] penguin [MASK] are flight Label = NotNext 첫번째 문장과 두번째 문장을 입력값으로 넣어 앞 뒤 문장이 연속되는 문장인지 분류하는 학습을 추가로 진행하였다. BERT 모델 Model Input 입력값의 형태는 하나의 문장이 입력값이 될 수도 있고, 두개의 문장 쌍 ( 예를 들면 Q&amp;A )가 입력값이 될 수 있다. e = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg) Token Embedding : Glove, Word2vec, Fasttext와 같은 것을 사용해 Vector 값을 가져온다. Segment Embedding : 단어가 첫번째 문장에 속하는지 두번째 문장에 속하는지 알려준다.[ 0 0 0 0 0 1 1 1 1 ] 과 같이 표현하며 해당 Vector를 Token Embedding 차원수와 같게 맞추어 임베딩 해준다. Positional Embedding : 각 단어가 첫번째인지 두번째인지를 의미하는 Embedding 값이다. 마찬가지로 Token Embedding의 차원수와 맟춰 임베딩 한 후 더해준다.​ Encoder Block Transformer는 encode-decoder 구조로 decoder에서 loss function을 계산해서 trainin을 진행한다. BERT에서는 Decoder 부분은 사용하지 않고 Encoder 부분만 사용을 한다. ( 붉은색 박스 부분 ) BERT는 N개의 encoder 블록을 사용한다. ( Base 모델은 12개, Large 모델은 24개로 구성 ) 인코더 블록의 수가 많을수록 단어 사이의 복잡한 관계를 더 잘 포찰할 수 있다.​ Pre-training Procedure 학습 데이터 : BooksCorpus(800M) + English Wikipedia(2,500M)위키 피디아의 경우 list, tables, header를 모두 제외하고 text만 사용 Batch size : 131,072 단어1Batch : 256 sequence x 512 words = 128,000 words 학습1Batch : 1024 sequence x 128 words = 131,072 words 학습 전체 step은 1,000,000번 으로 33억개의 단어 corpus에 대해서 대략 40 epoch 정도 학습 Dropout : 0.1 로 모든 레이어에 적용 Activate function : gelu BERT-Base : 12-layer, 768-hidden, 12-head BERT-Large : 24-layer, 1024-hidden, 16-head Find-tuning Procedure 4개 정도 type의 언어 모델링 task를 만들고 output layer의 형태만 바꿔서 핵심 엔진인 BERT의 parameter 공유한다. 기본적으로 Transformer Encoder 부분의 Weight 는 Freeze 하고 추가적으로 하나의 Fully Connected Layer를 설계하여 우리가 원하는 Label 에 맞게 훈련하는 형태로 진행한다. Result BERT 결과 종합 공부할 때 참고했던 URL [URL] : https://reniew.github.io/47/ 가장 Basic하게 설명되어 있는 페이지 [URL] : https://www.slideshare.net/deepseaswjh/rnn-bert 짧게 짧게 흐름 위주로 설명을 했다. [URL] : http://www.modulabs.co.kr/DeepLAB_Paper/21074 모두의 연구소 자료 구체적으로 예시를 들어 설명해서 좋았다 [URL] : http://hugrypiggykim.com/2018/12/09/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/ transformer에 대한 설명이 자세하게 되어 있어서 너무 좋았다. Attention 감 잡는데 굉장히 많은 도움이 되었다. [URL] : http://docs.likejazz.com/bert/ Basic 하게 설명되어 있다. [URL] : https://medium.com/ai-networkkr/%EC%B5%9C%EC%B2%A8%EB%8B%A8-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EC%86%94%EB%A3%A8%EC%85%98%EB%93%A4-1-%EA%B5%AC%EA%B8%80-bert-%EC%9D%B8%EA%B0%84%EB%B3%B4%EB%8B%A4-%EC%96%B8%EC%96%B4%EB%A5%BC-%EB%8D%94-%EC%9E%98-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-ai-%EB%AA%A8%EB%8D%B8-9704ebc016c4 글쓴이의 해석이 들어가 있다. 도움이 되었다. [URL] : https://www.facebook.com/groups/TensorFlowKR/permalink/767590103582050/ 정말 깔끔 담백하게 정리되어 있다. 처음에 봤을 때는 잘 이해가 되지 않았지만 이해하고 보니까 이렇게 담백하게 표현할 수 있을까 싶다. [URL] : https://github.com/huggingface/pytorch-pretrained-BERT#Fine-tuning-with-BERT-running-the-examples BERT를 사용해볼 수 있는 github 실제 돌려보니까 정말 이해가 잘 되었다. [URL] : http://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/ 코드와 함께 설명되어 있는 매우 희귀하고 좋은 자료 [URL] : https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3 BERT와 Transformer를 잘 설명했다. [URL] : http://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html 마찬가지로 코드와 함께 설명되어 있다.","link":"/2019/02/23/word_embedding/bert/"},{"title":"word2vec 실습","text":"서론 Word2vec에 대한 설명은 word_embedding/word2vec에 있다. 한겨레 신문의 정치 기사 대략 1000개를 수집하였다.위의 데이터를 사용해 word2vec을 구현한다. 코드는 github에 공개. [URL] : https://github.com/vhrehfdl/Blog/tree/master/word_embedding 총 3개의 python 파일로 구성되어 있다. ​​ 구성 make_corpus.py 한겨레 기사는 여러 문장으로 단락이 구성되어져 있다. 그래서 문장 by 문장으로 잘라서 정리를 한 후 corpus.txt라는 파일을 만들어준다. make_token.py 위에서 만든 corpus.txt라는 문장이 담겨있는 파일을 Token으로 변환시켜 corpus_token.txt라는 파일을 만든다. 영어는 NLTK를 활용하여 문장을 Token으로 나누어준다. 한국어는 형태소 분석기를 사용해 문장을 형태소 단위로 나누어 준다. 여기서는 Konlpy 라이브러리의 Komoran 모듈을 사용해 문장을 Token으로 변환시켜주었다. make_model.py Token으로 저장된 파일을 불러와 gensim의 word2vec에 입력한다. 결과 vector의 dimension은 100차원으로 정의하였다. 그래서 “서울”이라는 단어의 embedding은 아래와 같고유사한 단어는 경기도, 강남구, 중구 등이 나왔다. “김정일”이라는 단어를 넣으면 다음과 같은 결과가 나온다. ​","link":"/2019/02/20/word_embedding/word2vec_example/"},{"title":"word2vec","text":"정리 앞서 소개했던 BoW, TF-IDF는 Count Based Language Model에 속한다. 왜나하면 word를 vector로 표현할 때 단어의 빈도수를 특성으로 표현하기 때문이다. 이제부터 소개할 Language Model은 Neural Network Language Model이다. NNLM에는 word2vec, FASTTEXT, Glove와 같은 방법들이 존재한다. ​​ Count Based Language Model의 문제점 Problem 1 : 하나의 단어를 표현하는데 큰 벡터가 필요하다. 기존의 원 핫 인코딩에서 하나의 Column은 사전에 등록된 단어를 의미했다. 강아지 : [ 1 0 0 ] 멍멍이 : [ 0 1 0 ] 고양이 : [ 0 0 1 ] 만약 사전에 30,000개의 단어가 들어있다면 하나의 단어를 표현하기 위해서는 30,000 차원이 필요하다. ( Count Based LM은 듬성 듬성한 Sparse Vector의 모습을 가지게 된다. ) 큰 차원의 벡터는 계산복잡성이 크고 메모리 문제가 존재한다. 그리고 실제 위의 벡터에서 필요한 값은 1이 들어간 하나의 요소 뿐이고 나머지는 불필요한 0들만 포함되어 있다. Problem2 : 단어와의 관련성을 파악할 수 없다. 예를 들어 “강아지”와 멍멍이”가 유사하다는 것을 Count Based LM에서는 알 수 없었다. 저런 형태로는 관계를 전혀 파악할 수가 없다. 강아지 : [ 1 0 0 ] 멍멍이 : [ 0 1 0 ] 또한 위의 Vector를 내적하면 0이 된다. 길이가 1인 두 벡터의 내적은 두 벡터 사이의 각도가 되므로 두 벡터가 직교한다는 의미가 된다. 이를 확장해서 생각하면 원 핫 인코딩은 서로 독립한다는 것을 알 수 있다. 즉, 모든 단어들이 서로 영향을 미치지 않는 다는 것을 의미한다.​ Distributed Representation 위의 문제들을 해결하기 위해 Distributed Representation(분산 표상)을 사용한다. ​ ( 그림1 )을 보면 총 9개의 도형이 있다. 초록색 해 / 초록색 달 / 초록색 구름 파란색 해 / 파란색 달 / 파란색 구름 주황색 해 / 주황색 달 / 주황색 구름 만약 파란색 해를 One hot encoding으로 표현한다면 [ 0 0 0 1 0 0 0 0 0 ] 이 될 것이다. 위의 9차원 벡터는 2차원 벡터로도 표현 가능하다. 각 도형의 속성인 colors와 shapes를 조합하여 9개의 도형을 표현할 수 있기 때문이다. 분산표상은 데이터의 차원수를 줄여주기도 하지만 개체간 유사성을 비교할 수도 있게 만든다. 의미가 유사한 단어는 벡터 공간에서 가깝게, 반대의 경우에는 멀게 배치하는 것이 분산표상의 목표이다. Word2vec에서 단어 vector는 “강아지” = [ 0.2, 1.8, 2.8 … , -7.2 ]와 같이 표현된다. 이전의 Count-Based LM에서 vector의 column은 사전에 등록된 단어를 의미했다. 그렇다면 Word2vec에서 vector의 column은 위에서 언급한 colors, shapes와 같은 특성을 의미하는 것일까? 정답은 아니다. 그렇게 딱 딱 떨어지는 것이 아닌 colors와 shapes의 특성이 혼합된 것을 의미한다. 즉 “강아지”라는 단어 vector에서 0.2 가 동물, 1.8이 종류를 나타내는 것이 아니다. 여러 특성이 섞여서 수치로 표현 되는 것이다. “강아지” = [ 0.2, 1.8, 2.8 … , -7.2 ]와 같이 vector의 값이 dense(빽빽하게) 있기 때문에 dense vector라고도 한다. Neural Network Language Model 위에서 설명한 분산 표상을 가지는 Vector를 만들기 위해서 Shallow Neural Net 학습을 진행한다. Input Layer에는 One-Hot Encoding의 값이 들어간다.강아지 = [ 1 0 0 ] 의 값이 들어가는 것이다.x1 = 1 / x2 = 0 / x3 = 0 Hidden Layer에서 Weight와 Bias의 값이 존재한다. Hidden Layer의 노드 개수는 사용자가 정의한다. ( Hidden Layer의 노드 개수가 embedding dimention을 의미한다. ) 보통 128 / 256 / 512와 같이 2의 배수를 사용해서 정의한다. ( 딥러닝 학습 연산과 관련이 있다.) Output Layer에서는 Input Layer와 노드의 개수가 같다.(Input Layer와 Output Layer 노드 개수는 One-hot Encoding의 dictionary 개수와 같다.)출력값은 [ 0 1 0 ] 을 가지게 된다.y1 = 0 / y2 = 1 / y3 = 0멍멍이 = [ 0 1 0 ] NNLM의 학습은 다음 단어가 무엇인지를 예측하는 방식으로 이루어진다. “The fat cat sat on the mat” 이라는 문장이 있다.Input에는 The = [ 1 0 0 0 0 0 0 ] 이 들어가고 hidden layer를 거쳐서 predict 한 값이 [ 0 1 0 0 0 0 0 ]이 되도록 학습을 진행하다는 것이다. 이렇게 학습을 진행하면 단어간의 의미적인 관계가 담겨있는 weight vector가 생성된다는 것이 NNLM의 기본개념이다. CBOW와 Skip-gram을 설명하면서 더 자세하게 설명하겠다. CBOW NNLM의 학습 방법은 CBOW와 Skip-Gram 방법이 존재한다. CBOW는 주변 단어를 사용해 중간 단어를 예측하는 방법을 의미한다. ​ (그림3)을 보면 중심 단어를 기준으로 앞뒤 2개의 단어를 학습데이터로 사용을 한다. 주변 단어를 Input에 넣고 Hidden Layer를 거쳐서 Output Layer에서 나온 값이 중심 단어가 되도록 학습시키는 것이다. ​ (그림4)는 (그림3)을 이해하기 쉽게 도식화한 이미지이다. sat이라는 중심단어를 예측하기 위해 fat, cat, on, the와 같은 주변 단어를 입력하였다. ​ (그림5)를 보면 알 수 있듯이 4개의 벡터 값의 평균을 구하여 해당 Vector 값을 Word Vector로 사용한다. ​​ Skip-Gram Skip-gram은 CBOW와 반대로 중심단어를 사용해 주변단어를 예측하는 것이다.보통 CBOW보다 Skip-gram을 많이 사용한다고 하는데 이유는 다음과 같다. CBOW에서는 주변단어를 4개를 사용해 predict를 하고 한 번만 역전파를 한다.하지만 skip-gram에서는 중심단어 1개를 사용해 predict를 하고 네 번 역전파를 진행한다. 즉, skip-gram이 역전파하는 회수가 CBOW보다 더 많기 때문에 학습이 잘 된다고 한다. ​​​ 정리 word2vec을 이용해 단어의 vector를 구한다는 것은 NNLM의 신경망에서 학습된 가중치 벡터를 가져오는 것이다. ​ ( 그림7 ) 과 같이 좌측 행렬은 단어를 의미하는 행렬이고 우측 행렬은 weight vector이다. “강아지”라는 단어는 [ 10 12 19 ] 라는 벡터로 표현되는 것이다.이것은 weight vector의 행이 단어의 벡터를 의미한다고 볼 수 있다. 왜냐하면 [ 0 0 0 1 0 ]에서 보는 것처럼 0과 곱해지면서 1이 포함된 weight vector의 행만 출력이 되기 때문이다.(말로 잘 설명이 안되네요… 행렬의 곱셈을 할 줄 아는 사람이라면 충분히 이해할 것이라 생각합니다.) 그렇다면 weight vector는 어디서 가져올까? wiki-docs 와 같은 데이터를 Pre-trained한 Weight vector 값을 가져올 수 있다. 아니면 자신의 목적에 맞게 학습을 시켜 Weight vector를 생성하고 그 vector 값을 가져와 사용할 수 있다. wiki-docs에서 사전에 학습한 weight vector 값을 가져온다고 가정해보자.wiki-docs가 많은 단어를 가지고 있기는 하지만 내가 사용하는 데이터의 단어를 가지고 있지 않은 경우도있을 것이다. 그런 경우에는 해당 단어의 vector 값을 random 하게 부여하기도 하고 값을 크게 부여하기도 한다.자기가 설계하기 나름이다. 기존의 Count Based LM에서는 사전의 단어 개수가 차원수 였다면 NNLM에서는 사용자가embedding_dim을 몇으로 정의하는가에 따라 단어의 차원수가 정해진다. ( 처음에는 이게 이해가 안 되서 고생을 했다. ) ​ 공부할 때 참고했던 URL [URL] : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/ word2vec 학습 방식 개괄적으로 잘 설명해두었다.​ [URL] : https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/29/NNLM/ Distributed Representation 잘 설명 Neural Network Language Model을 설명 [URL] : https://wikidocs.net/22660 NNLM의 학습방법에 대해 매우 자세히 설명되어 있다. [URL] : https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html Count-Based LM 부터 NNLM의 방법까지 잘 설명되어 있다. [URL] : https://lovit.github.io/nlp/2018/04/05/space_odyssey_of_word2vec/ 코드와 구체적인 예시등이 설명되어 있다. [URL] : https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/word2vec/ 텐서플로우 코리아 word2vec 문서","link":"/2019/02/19/word_embedding/word2vec/"},{"title":"EC2 Instance 웹서버 설치 후 구동하기","text":"EC2 Webserver 설치 후 구동 기록. ​ Problem Nginx를 설치하고 IPv4 퍼블릭 IP 주소 값으로 접근을 했는데 되지 않았다. Solution 방화벽에 80번 포트를 추가해주어야 한다. Descripttion에 Security group의 launch-wizard를 선택한다. Inbound 탭을 누른다. Edit 버튼을 누르고 Add Rule 버튼을 클릭한다. 이렇게 추가하고 들어가면 끝! 접속이 될 것 이다. 참고 URL https://medium.com/@taeyeolkim/aws-ec2%EC%97%90-%EC%9B%B9%EC%84%9C%EB%B2%84-nginx-%EC%84%A4%EC%B9%98%ED%95%98%EA%B3%A0-%EA%B5%AC%EB%8F%99%ED%95%98%EA%B8%B0-a46a6e9484a8","link":"/2019/11/08/setting/aws_port_open/"},{"title":"EC2 Instance 생성 후 putty 접속","text":"EC2에 putty로 접속하는 방법 기록. ​ AWS EC2 Instance 생성 [URL] : https://aws.amazon.com/ko/ 콘솔에 로그인 하기를 한 후 EC2 인스턴스를 생성한다. 사실 생성까지는 쉽지만 약간 어려울 수 있는게 보안 그룹에 Key File 부분이 헷갈릴 수 있다. 우선 .pem 확장자 파일을 다운 받는다. ( 접속 때 중요하게 사용되어진다. ) AWS putty 접속 우선 puttygen을 사용해 .pem 파일을 .ppk 파일로 생성한다. Load 버튼을 클릭한 후에 방금 만들었던 .pem 파일을 선택한다. save 버튼을 누르고 이름을 지정한 후 저장한다. Host Name에는 퍼블릭 DNS(IPv4)의 주소값을 입력한다. 접속&gt;SSH&gt;AUTH로 가서 Private Key파일을 선택한다. .ppk 파일을 세팅하고 접속한다. EC2에 접속하면 “login as:”가 화면에 뜨는데 우분투로 설치했다면 ubuntu로 입력한다. ( 처음에 아무생각 없이 root를 했다가 안되었다. ) 참고 URL [URL] : https://hyeonstorage.tistory.com/271","link":"/2019/11/08/setting/aws_putty/"},{"title":"Docker 설치하기","text":"Docker 설치와 기본 명령어 기록. ​ Docker 설치 curl -fsSL https://get.docker.com/ | sudo sh ​ 설치 확인 docker version 설치가 완료되면 아래과 같이 결과가 나온다. ​ docker 명령어 모음 Image 파일 설치 : docker run ubuntu:16.04 docker run docker run ubuntu:16.04 docker exec : 실행중인 컨테이너에 접속할 때 사용 docker exec -it mysql bash docker image 확인 docker images docker container 확인 docker ps -a docker image 제거 docker rmi 'image id' docker container 제거 docker rm 'conatiner id' ​ 참고한 URL [URL] : https://subicura.com/2017/01/19/docker-guide-for-beginners-2.html 매우 잘 설명 되어 있고 예제 코드도 많다. [URL] : http://pyrasis.com/book/DockerForTheReallyImpatient/Chapter20/08 Exec 명령어 설명되어 있다. [URL] : https://blusky10.tistory.com/362 docker mysql 설치할 때 참조했다. [URL] : https://behonestar.tistory.com/85 docker 전체 삭제 명령어 [URL] : https://blog.hanumoka.net/2018/04/29/docker-20180429-docker-install-mysql/ 마찬가지로 mysql 설치할 때 참조했다.​","link":"/2019/02/24/setting/docker_install/"},{"title":"Google Colab 사용법 정리","text":"colab 기초 사용법 ​ Tutorial Colab에 접속한다. [URL] : https://colab.research.google.com/notebooks/welcome.ipynb​ 새로운 python3.ipynb 파일을 만든 후 google drive mount를 한다. 해당 코드를 실행하면 enter authorization code를 입력하라고 창이 뜬다. from google.colab import drivedrive.mount('/content/gdrive') authorization code 받는 주소 [URL] : https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH​ 해당 URL에 들어가면 최종적으로 authorization code를 받을 수 있다. foo.txt 파일을 드라이브에 업로드 with open('/content/gdrive/My Drive/foo.txt', 'w') as f: f.write('Hello Google Drive!')!cat /content/gdrive/My\\ Drive/foo.txt ​","link":"/2019/03/22/setting/colab_tutorial/"},{"title":"Google Cloud Storage 설치하기","text":"Google Cloud Storage 설치 ​ 설치 과정 curl https://sdk.cloud.google.com | bashexec -l $SHELLgcloud init ​ 새로운 버킷 생성 명령어 gsutil mb -l us-east1 gs://my-awesome-bucket/ ​ 파일 업로드 gsutil cp Desktop/kitten.png gs://my-awesome-bucketgsutil cp /home/lee/video/chul9hyung/5Hj3sW2OroU.wav gs://my-youtube-bucket ​​ 버킷에 파일 조회 gsutil ls gs://my-awesome-bucket","link":"/2019/02/14/setting/google_cloud_storage/"},{"title":"Keras GPU 설치","text":"후기 Keras는 Tensorflow랑 다르게 크게 변경할 것이 없다.Keras는 정말 사용하기 쉬운 것 같다. ​ 설치 anaconda 에 가상 환경 만들어주기 anaconda prompt에서 다음의 명령어를 실행해준다. conda create --name kerasactivate keras keras-gpu 버전을 설치한다. conda install keras-gpu ​ CPU와 GPU 속도 비교 CPU : Intel(R) Core(TM) i7-8700K CPU @ 3.7GHz GPU : NVIDIA GeForce GTX 1080Ti 사용 코드 : CNN을 이용한 MNIST 데이터 분류 코드 '''Trains a simple convnet on the MNIST dataset.Gets to 99.25% test accuracy after 12 epochs(there is still a lot of margin for parameter tuning).16 seconds per epoch on a GRID K520 GPU.'''from __future__ import print_functionimport kerasfrom keras.datasets import mnistfrom keras.models import Sequentialfrom keras.layers import Dense, Dropout, Flattenfrom keras.layers import Conv2D, MaxPooling2Dfrom keras import backend as Kbatch_size = 128 # 배치 사이즈 128로 설정.num_classes = 10 # 정답 라벨링 개수 ( 0 ~ 10 )epochs = 3 # Epoch 반복 회숫 3회로 설정# input image dimensionsimg_rows, img_cols = 28, 28 # 이미지 가로 세로 배열.# MNIST 데이터 불러오기(x_train, y_train), (x_test, y_test) = mnist.load_data()if K.image_data_format() == 'channels_first': x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols) x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols) input_shape = (1, img_rows, img_cols)else: x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1) x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1) input_shape = (img_rows, img_cols, 1)x_train = x_train.astype('float32')x_test = x_test.astype('float32')x_train /= 255x_test /= 255print('x_train shape:', x_train.shape)print(x_train.shape[0], 'train samples')print(x_test.shape[0], 'test samples')# convert class vectors to binary class matricesy_train = keras.utils.to_categorical(y_train, num_classes)y_test = keras.utils.to_categorical(y_test, num_classes)model = Sequential()model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape)) # Input Layer : 32개의 필터를 가진다.model.add(Conv2D(64, (3, 3), activation='relu')) # 첫번째 hidden layer이며 64개의 필터를 가진다.model.add(MaxPooling2D(pool_size=(2, 2))) # Pooling Layer를 의미한다.model.add(Dropout(0.25)) # Dropout을 0.25로 하여 overfitting을 방지한다.model.add(Flatten()) # 데이터를 1차원으로 바꿔주는 Layer. 아래의 fully_connected_layer에 연결해주기 위해 1차원으로 바꾸어준다.model.add(Dense(128, activation='relu')) # Dense는 Fully Connected Layer를 의미한다.model.add(Dropout(0.5)) # Dropout은 0.5로 하여 overfitting을 방지한다.model.add(Dense(num_classes, activation='softmax')) # Output Layer를 의미하며 activation은 softmax로 분류흘 한다.model.summary() # 모델의 요약을 보여준다.model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))score = model.evaluate(x_test, y_test, verbose=0)print('Test loss:', score[0])print('Test accuracy:', score[1]) CPU 결과 : 167초 GPU 결과 : 9초 ​ 참고 URL [URL] : https://agiantmind.tistory.com/192","link":"/2019/01/16/setting/keras_gpu_install/"},{"title":"Konlpy 설치","text":"Konlpy install 방법. 설치 Python 환경을 설정해준다. pip 환경을 설치해준다. python 2.x 버전과 3.x 버전의 설치 명령어가 다르다. apt-get install python-pip # 2.x versionapt-get install python3-pip # 3.x version Konlpy 설치 python 2버전과 3버전의 설치 명령어가 다르다. apt-get install python-dev # 2.x versionpip install konlpyapt-get install python3-dev # 3.x versionpip3 install konlpy ​ Java 설치 Konlpy를 실행시키기 위해서 Java가 필요하다. apt-get install openjdk-8-jdk ​ Jpype Error java-open-jdk가 설치되었는지 확인. 설치 안되면 다시 설치! 환경변수를 설정해준다. [참고 URL] : https://zetawiki.com/wiki/%EB%A6%AC%EB%88%85%EC%8A%A4_$JAVA_HOME_%ED%99%98%EA%B2%BD%EB%B3%80%EC%88%98_%EC%84%A4%EC%A0%95 ​ jpype를 설치한다. $ sudo apt-get install g++ openjdk-8-jdk python-dev python3-dev$ pip install JPype1 # in Python 2.x$ pip3 install JPype1-py3 # in Python 3.x ​ 메캅 설치 에러 mecab 설치 제대로 안되었을 때 bash에서 설치해주어야 한다. 정말 안되면은 autoconf를 해준다음 다시 설치해야 한다. $ sudo apt-get install curl$ bash &lt;(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)apt-get install -y autoconf #중요해!!자바 heap 메모리 부족할 때 자바 옵션을 건드리면 된다. export _JAVA_OPTIONS=-Xmx4096m","link":"/2019/01/18/setting/konlpy_install/"},{"title":"Ubuntu git 설치","text":"Git Install ​ 설치 Git 설치하기 apt-get install git-core Git 환경설정하기 git config --global user.name \"UserName\"git config --global user.email UserEmail Git 추가하기 git add . Git commit 하기 git commit -m \"upload server\" ```​​5. 기존에 git에 있는 코드를 가져온다. ``` bash #git에 있는 코드를 땡겨온다. 자동으로 merge가 된다. git pull origin master Git 서버에 올린다. #현재 server 코드를 upload 해준다.git push origin master ​","link":"/2019/02/07/setting/linux_git/"},{"title":"MySQL 백업 복원","text":"MySQL Install. ​ 백업 DB별로 백업 mysqldump -u root -p DB명 &gt; 파일명.sql ​ 전체 백업 mysqldump -u root -p –all-databases &gt; 파일명.sql ​ 캐릭터셋 옵션을 이용하여 백업 mysqldump -u root -p –default-character-set=euckr DB명 &gt; 파일명.sql​ ​ 특정 테이블만 덤프 mysqldump -u root -p DB명 테이블명 &gt; 파일명.sqlmysqldump -u root -p -B DB명 –tables 테이블명1 테이블명2 테이블명3 &gt; 파일명.sql ​ 테이블 구조만 백업 mysqldump -u root -p –no-data DB명 &gt; 파일명.sql ​ XML 파일로 백업 mysqldump -u root -p –xml DB명 &gt; 파일명.sql ​ 복구 DB별로 복구 mysql -u root -p DB명 &lt; 파일명.sql 전체 복구 mysql -u root -p &lt; 파일명.sql ​ 캐릭터셋 옵션을 이용하여 백업 복구 mysqldump -u root -p –default-character-set=euckr DB명 &lt; 파일명.sql ​","link":"/2019/02/07/setting/mysql_backup/"},{"title":"Nginx + Flask + Uwsgi 초기 세팅","text":"Nginx + Flask + Uwsgi 초기 세팅 기록. ​ 설치 및 실행 Nginx, Flask, Uwsgi를 설치한다. pip3 install nginxpip3 install uwsgipip3 install flaskpip3 install uwsgi-plugin-python Flask 실행 test.py 라는 파일을 만들고 실행하여 결과를 확인한다. from flask import Flaskapplication = Flask(__name__)@application.route(\"/\")def hello(): return \"&lt;h1&gt;Hello!&lt;/h1&gt;\"if __name__ == \"__main__\": application.run(host='0.0.0.0') curl 명령어를 실행해서 확인해본다. curl http://localhost:5000 uwsgi 파일 만들기 from test import applicationif __name__ == \"__main__\": application.run() curl 명령어를 실행시켜 확인 uwsgi --socket 0.0.0.0:5000 --protocol=http -w wsgi ini 파일을 만든다. 매번 uwsgi 파일을 실행시킬 때 명령어를 써줄 수 없으니 파일을 만들어둔다. socket 파일은 아래 실행 명령어를 실행시키면 자동으로 생성 되어진다. [uwsgi]chdir = /var/www/html/StoryAI/storyai/webmodule = wsgisocket = /tmp/myproject.sockchmod-socket = 666 실행 명령어 uwsgi --ini myproject.ini nginx의 환경설정을 변경한다. /etc/nginx/sites-avilable/defalut 다른 예제들에서는 location @app을 설정해서 하는데 이거 인식이 잘 안되서 location / { 부분에 설정을 해주니까 되었다. sock 파일을 통해서 서로 연결되어진다. 여기서 실수 했던게 try_files $url $url\\ =404; 부분을 주석처리 안하니까 app.route를 다른걸 설정하니까 접속이 되지 않았다. 뻘짓했다. 참고 사이트 [URL] : https://cjh5414.github.io/flask-uwsgi-nginx/ [URL] : https://whatisthenext.tistory.com/124 [URL] : https://twpower.github.io/43-run-uwsgi-by-using-ini-file","link":"/2019/11/09/setting/nginx_flask_uwsgi/"},{"title":"mysql window에서 설치","text":"후기 쉽게 설치할 수 있는 것이기는 하지만 혹시 나중에 또 설치할 때 귀찮을까봐 미리 정리해둔다. 설치 mysql installer 설치 zip파일로 설치하는 과거 문서들이 많은데 installer로 설치하는게 훨씬 간편하다. [Download] : https://dev.mysql.com/downloads/windows/installer/ Installer 실행 후에는 다른거 건드리지 말고 그냥 next만 누르면 된다. 중간에 root 비밀번호 입력하라고 나오는데 그것만 입력하고 계속 next 누르면 설치 완료된다. mysql 윈도우 cmd에서 사용하기 환경변수를 설정하지 않으면 ( 그림1 ) 처럼 mysql -u root -p 했을 때 없는 명령어라고 뜬다. 환경변수 설정은 시스템-&gt; 정보 -&gt; 시스템 정보 -&gt; 고급 시스템 설정 환경변수 -&gt; 시스템 변수 ( Path ) -&gt; 편집 ​ mysql 폴더 내부의 bin 폴더 위치를 환경변수의 등록해주면 마무리 된다.","link":"/2019/01/16/setting/mysql_window/"},{"title":"Pycharm 가상환경 설정","text":"후기간단하다. 그냥 혹시라도 나중에 찾아볼까 만들어둔다.다만 Pycharm이 처음이고해서 낯선 감이 없지 않아 있지만 Jetbrain 계열이라 친숙하다. 설치 Pycharm을 설치한다. 다행히 학교 계정이 있어서 무료로 사용 가능했다. [Download] : https://www.jetbrains.com/pycharm/download/#section=windows Anaconda에 기존에 설정해둔 가상환경을 Pycharm에서 사용한다. New Project에 들어가고 Existing Interpreter에 들어가 Interpreter 부분을 들어가 찾으면 된다. 아나콘다 가상환경 밑에 있는 python.exe 파일을 입력해주면 된다. ​Anaconda의 가상환경 파일이 어디있는지 못 찾았었는데 ../Anaconda3/envs/자신의 가상환경/python.exe 여기에 있다. 참고 URL [URL] : https://steemkr.com/kr-dev/@maanya/3ajlbo","link":"/2019/01/16/setting/pycharm_setting/"},{"title":"Nginx + PHP7.0 + MySQL 설치","text":"후기 : Nginx + PHP7.0 + MySQL 설치 과정 설명 설치 Nginx 설치 apt-get install nginx 설치 확인 IP주소로 들어가면 확인된다. PHP7.0 설치 apt-get install php7.0-fpmapt-get install php7.0-gd php7.0-curl php7.0-mbstring php7.0-xml php7.0-mcrypt php.ini 파일 설정 /etc/nginx/sites-avilable/default 로 이동 index index.html index.htm 있는 부분에 index.php 추가 밑에 처럼 PHP 관련 주석 풀어주기. fastcig_pass는 일단 풀지말아야 한다 error 난다.​ php 설치 확인 MySQL 설치 apt-get install mysql-server mysql-client apt-get install php7.0-mysql mysql이랑 PHP7.0 연동해주는 코드이다. 이거 안해서 삽질했다. 이거 안하면 php.ini가서 extension 설정 바꿔주고 경로도 다 바꾸어주어야 한다. ​ MySQL 연동 확인 &lt;!DOCTYPE html&gt;&lt;head&gt; &lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"/&gt; &lt;title&gt;MySql-PHP 연결 테스트&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;?php echo \"MySql 연결 테스트\";$db = mysqli_connect(\"localhost\", \"root\", \"비밀번호\", \"world\");if($db){ echo \"connect : 성공\";}else{ echo \"disconnect : 실패\";}$result = mysqli_query($db, 'SELECT VERSION() as VERSION');$data = mysqli_fetch_assoc($result);echo $data['VERSION'];?&gt;&lt;/body&gt;&lt;/html&gt; ​​ 아래처럼 뜨면 확인 끝 ​","link":"/2019/01/16/setting/nginx_php_mysql_install/"},{"title":"PostgreSQL 백업 복원","text":"PostgreSQL은 DB 특성상 console 작업이 너무 불편해 PgAdmin을 연동해야 한다. ​ PgAdmin 연동 PostgreSQL 외부접속을 허용한다. listen_addresses 변경 /etc/postgrsql/9.5/main/postgresql.conf 에 들어간다. 저 부분을 변경해준다. host 값 추가 /etc/postgresql/9.5/main/pg_hba.conf 에 들어가 가장 마지막 부분에 한 줄 입력해준다. DB 재시작 백업 Backup을 눌러준다. 복구 Restore을 클릭해준다. 참고 URL [URL] : http://printhelloworld.tistory.com/143 ​","link":"/2019/02/07/setting/postgresql_backup/"},{"title":"Pycharm에서 scrapy 설치 및 실행하기","text":"후기 : Local에 VisualBasic C++ 관련 라이브러리 들이 설치되어 있어야 정상적으로 pycharm에서 scrapy를 설치할 수 있다. ​ 설치 Scrapy 설치 Setting을 들어가 라이브러리 추가하는 곳에서 scrapy를 추가한다. 만약, 추가했을 때 에러가 발생한다면 비쥬얼 베이직 프로그램 설치하고 C++ 관련 라이브러리를 추가해야 한다. import scrapy Scrapy 커맨드 실행 from scrapy.cmdline import executeexecute() Scrapy 프로젝트 생성 import scrapyfrom scrapy import cmdlinecmdline.execute(\"scrapy startproject naverCrawler\".split()) ​ 참고 URL [URL] : https://krksap.tistory.com/1053 ​","link":"/2019/01/30/setting/pycharm_scrapy_install/"},{"title":"Pytube Sample Code","text":"Python을 이용한 Youtube 다운로드 라이브러리. ​ pip 툴을 사용해 pytube를 install 한다. code를 돌려 youtube 영상을 다운 받는다. # -*- coding: utf-8 -*-import osimport subprocessimport pytubeyt = pytube.YouTube(\"https://www.youtube.com/watch?v=ZMpQgvmw3kk\") #다운받을 동영상 URL 지정vids= yt.streams.all()#영상 형식 리스트 확인for i in range(len(vids)): print(i,'. ',vids[i])vnum = int(input(\"다운 받을 화질은? \"))parent_dir = \"/home/lee\" #저장 경로 지정(Windows or mac)vids[vnum].download(parent_dir) #다운로드 수행default_filename = vids[vnum].default_filenamesubprocess.call(['ffmpeg', '-i', #cmd 명령어 수행 os.path.join(parent_dir, default_filename)])print('동영상 다운로드 및 mp3 변환 완료!') ​","link":"/2019/02/25/setting/pytube_sample/"},{"title":"SSH 접속","text":"SSH 접속 설정하는 법. 1.openssh를 설치해준다.apt-get install openssh-server 2. 설정파일을 바꾸어준다./etc/ssh/sshd_config 파일의 PermitRootLogin 부분을 yes로 만들어준다. 3. 비밀번호를 설정해준다.root 비밀번호가 처음에는 아마 설정 안되어 있기 때문에 설정해준 다음 접속한다. sudo passwd root 4. ssh를 재시작한다.sudo service ssh start ! 번외근데 최근에 ubuntu 18 버전으로 설치하고 openssh-server install 했는데 sshd_config 파일이 전부 주석처리 되어 있는 경우가 있었다. 결국은 해결 못 하고 16.04버전으로 했는데… 이럴 경우에 어떻게 해야 할지 좀 찾아봐야 겠다.","link":"/2019/01/15/setting/ssh_connect/"},{"title":"Tensorboard 사용법","text":"생각 처음에 아무 생각 없이 하다가 내 tensorflow를 아나콘다 가상환경에 설치한 사실을 깜빡하고 tensorboard를 실행 못시켰었다. tensorflow가 설치되어 있는 가상환경을 activate하고 tensorboard를 실행시키자. ㅋㅋ ​ 설치 및 실행 tensorboard 설치 기본적으로 tensorflow 설치하면 자동으로 tensorboard가 설치되어진다. mygraph라는 폴더를 만들어서 log를 저장한다. import tensorflow as tfa = tf.constant(5, name='input_a')b = tf.constant(7, name='input_b')c = tf.multiply(a, b, name='mul_c')d = tf.add(a, b, name='add_d')e = tf.add(c, d, name='add_e')sess = tf.Session()print(sess.run(e))wirter = tf.summary.FileWriter('./mygraph',sess.graph) mygraph가 위치하는 폴더에 가서 아래의 명령어를 실행시켜준다. tensorboard --logdir=\"mygraph\" 커맨드창 가장 하단에 있는 http://localhost:6006으로 들어가면 tensorboard가 실행되어진다. 최종 결과 ​ 응용 아래의 코드는 callback 함수로 만들어서 log를 쌓는 방법이다. from keras.callbacks import TensorBoarddef create_callbacks(): tensorboard_callback = TensorBoard(log_dir='./mygraph/mlp', histogram_freq=1, batch_size=32, write_graph=True, write_grads=False) return [tensorboard_callback] 위의 코드를 만든후에 model을 실행시킬 때 콜백으로 넣어주면 log가 실시간으로 쌓인다. callbacks = create_callbacks()model.fit(x=data[\"train_X\"], y=data[\"train_y\"], batch_size=32, epochs=200, verbose=1, validation_data=(data[\"val_X\"], data[\"val_y\"]), callbacks=callbacks)","link":"/2019/03/08/setting/tensorboard_setting/"},{"title":"Tensorflow gpu 설치","text":"Tensorflow gpu 설치 기록! 2019.12.15 업데이트 최근에 연구실 컴퓨터 바꾸면서 GPU 세팅 다시했는데 금방 환경 세팅이 된 것 같다. CUDA Toolkit 9.X 버전 설치 CuDNN 9.X 버전대 맞춰서 설치 CuDNN 압축 해제 Window 환경변수에 CuDNN bin 위치 추가 Anaconda로 가상환경 설정 후 tensorflow gpu 1.5 버전 설치(근데 이 부분이 좀 헷갈리는게 Anaconda Navigator에서 tensorflow gpu 설치하면 자동으로 CUDA 버전에 맞춰서 tesnsorflow gpu가 1.5로 세팅이 되는 것 같다. 정확히 확인은 못 했는데 분명 2.0버전으로 설치했는데 확인해보니 1.5가 설치되어 있었다. 뭐지???) CUDA 버전, CuDNN 버전, Tensorflow 버전만 잘 맞추면 큰 문제 없이 설치 할 수 있다.window 환경변수는 CuDNN bin 폴더만 잘 설정하면 된다.별 거 없다. (자주 해서 그런가?) 후기 설치는 했는데 갑자기 성공해서 왜 성공한지를 모르겠다. 추측컨데, 첫번째 이유로 CUDA Toolkit과 cuDNN 부분의 호환 문제인 것 같다. 두번재 이유로는 환경 설정 문제인 것으로 추정되어진다. ( 추가. tensorlfow gpu 1.5 버전으로 설치해서 성공한거다. 최신 버전 설치했을 때는 DLL 에러 발생했었는데… 1.5 버전으로 하니까 된다. )​​ 설치 CUDA Toolkit 설치 CUDA Toolkit 9.0을 다운로드 했다. ( cuDNN 과의 호환 때문에 버전을 신경써서 설치해야 한다. 가급적이면 똑같이 하는게 좋다. ) [Download] : https://developer.nvidia.com/cuda-90-download-archive?target_os=Windows&amp;target_arch=x86_64&amp;target_version=10&amp;target_type=exelocal 자신의 컴퓨터 사양과 맞는 것을 클릭하고 다운로드 받는다.​ cuDNN 설치 7.0.5 버전을 다운로드 받았다. [Download] : https://developer.nvidia.com/rdp/cudnn-archive ​ ​ 환경변수 설정 여기 부분에서 삽질을 한 것 같다.이게 처음에 다른 글들 보고 하다가 TOOLKIT 10점대 버전 설치하고 8점대 버전 설치하고 최종적으로 9점대 버전을 설치했다.그러다보니 밑에 CUDA_PATH_V8_0, CUDA_PATH_V10_0 등이 생기면서 경로를 못 잡는 상황이 발생했었다.( ImportError: DLL load failed ) 이런 에러가 발생한다면 환경변수 설정을 잘 못해서 발생하는 에러이다.​ 그냥 딱 더도 말고 덜도 말고 CUDA_PATH와 CUDA_PATH_V9_0만 확인하면 된다.괜히 사용자 변수 편집 버튼 눌러서 이상한 거 추가하다 더 안된 것 같다. 환경 변수 설정은 밑에 페이지에 잘 설명되어 있다. [참고 URL] : https://m.blog.naver.com/chandong83/221112939772 ​ Anaconda 설치 밑의 URL에서 Python 3점대 버전을 설치해준다.가급적이면 Anaconda를 설치해서 환경을 분리해주는 것이 좋다.안 그러면 환경설정이 엉켜서 나중에 감당할 수 없게 된다. [Download] : https://www.anaconda.com/download/ anaconda prompt 창에 들어가 아래 명령어를 입력한다. conda create -n cuda pip python=3.5 /*환경 만들기(python 3.5) -&gt; y/n 나오면 y 누를것.*/activate cuda /*환경 활성화*/python -m pip install --upgrade pip /*일단 pip 를 업그레이드 시켜준다.*/pip install --ignore-installed --upgrade tensorflow-gpu==1.5 중요 tensorflow-gpu 1.5 버전 설치해야 한다. (1.5버전 이상으로 하려면 CUDA나 CuDNN 버전까지 상향해서 설치해야 한다.)최신 버전 설치하면 DLL 에러 발생한다!!! ​ jupyter notebook 설치 pip3 install jupyter jupyter notebook 실행 한 후에 테스트 하면 끝나게 된다. ​ 테스트 밑의 코드를 실행시키고 에러가 발생하지 않는다면 설치에 성공한 것이다. &gt;&gt;&gt; import tensorflow as tf&gt;&gt;&gt; hello = tf.constant(‘Hello, TensorFlow!’)&gt;&gt;&gt; sess = tf.Session()&gt;&gt;&gt; sess.run(hello)b’Hello, TensorFlow!’&gt;&gt;&gt; a = tf.constant(10)&gt;&gt;&gt; b = tf.constant(32)&gt;&gt;&gt; sess.run(a + b)42 ​ 참고 URL 가장 큰 도움이 되었다. ( 구세주… ) https://devyurim.github.io/python/tensorflow/2018/04/30/tensorflow-1.html 처음에 이거 보고 했는데… 잘 안되었다. http://solarisailab.com/archives/1581 이거 보고 하다가 가장 크게 엉킨 것 같다. https://m.blog.naver.com/chandong83/221112939772 ​","link":"/2019/01/15/setting/tensorflow_gpu_setting/"},{"title":"ubuntu 3306 port 열기","text":"가장 기본적인 3306 포트 여는 방법 1. mysql 폴더로 이동cd /etc/mysql/ 2. mysql.conf에서 mysql.cnf를 들어간다.vi mysql.conf.d 3. bind-address = 127.0.0.1 부분을 주석처리한다.# bind-address = 127.0.0.1 4. mysql 들어가서 설정을 바꿔준다.grant all privileges on *.* to 'root'@'%' identified by '비밀번호';flush privileges; 5. mysql restart를 한다.sudo /etc/init.d/mysql restart 6. 테스트 해본다.mysql -h \"IP주소\" -P 3306 -u root -p cf. 이래도 안되면은 방화벽을 검사해봐야 한다.","link":"/2019/01/15/setting/ubuntu_open_port/"}],"tags":[{"name":"google cloud","slug":"google-cloud","link":"/tags/google-cloud/"},{"name":"google speech api","slug":"google-speech-api","link":"/tags/google-speech-api/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"encoding","slug":"encoding","link":"/tags/encoding/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"},{"name":"pickle","slug":"pickle","link":"/tags/pickle/"},{"name":"pycharm","slug":"pycharm","link":"/tags/pycharm/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"exec","slug":"exec","link":"/tags/exec/"},{"name":"scrapy","slug":"scrapy","link":"/tags/scrapy/"},{"name":"tensorflow gpu","slug":"tensorflow-gpu","link":"/tags/tensorflow-gpu/"},{"name":"tensorflow hub","slug":"tensorflow-hub","link":"/tags/tensorflow-hub/"},{"name":"elmo","slug":"elmo","link":"/tags/elmo/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"tensorflow","slug":"tensorflow","link":"/tags/tensorflow/"},{"name":"drop_remainder","slug":"drop-remainder","link":"/tags/drop-remainder/"},{"name":"ubuntu","slug":"ubuntu","link":"/tags/ubuntu/"},{"name":"bow","slug":"bow","link":"/tags/bow/"},{"name":"one hot encoding","slug":"one-hot-encoding","link":"/tags/one-hot-encoding/"},{"name":"tf idf","slug":"tf-idf","link":"/tags/tf-idf/"},{"name":"bert","slug":"bert","link":"/tags/bert/"},{"name":"word2vec","slug":"word2vec","link":"/tags/word2vec/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"putty","slug":"putty","link":"/tags/putty/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"colab","slug":"colab","link":"/tags/colab/"},{"name":"gcs","slug":"gcs","link":"/tags/gcs/"},{"name":"keras","slug":"keras","link":"/tags/keras/"},{"name":"gpu","slug":"gpu","link":"/tags/gpu/"},{"name":"konlpy","slug":"konlpy","link":"/tags/konlpy/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"flask","slug":"flask","link":"/tags/flask/"},{"name":"uwsgi","slug":"uwsgi","link":"/tags/uwsgi/"},{"name":"window","slug":"window","link":"/tags/window/"},{"name":"postgresql","slug":"postgresql","link":"/tags/postgresql/"},{"name":"pytube","slug":"pytube","link":"/tags/pytube/"},{"name":"port","slug":"port","link":"/tags/port/"},{"name":"tensorboard","slug":"tensorboard","link":"/tags/tensorboard/"}],"categories":[{"name":"Error","slug":"Error","link":"/categories/Error/"},{"name":"word embedding","slug":"word-embedding","link":"/categories/word-embedding/"},{"name":"setting","slug":"setting","link":"/categories/setting/"}]}